/data/cl/u/nsarkar/miniconda/envs/litgpt/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data/cl/u/nsarkar/miniconda/envs/litgpt/bin/litgpt  ...
Using bfloat16 Automatic Mixed Precision (AMP)
wandb: Currently logged in as: nandan-sarkar (pcfg_pretrain). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.2
wandb: Run data is saved locally in ./wandb/run-20240708_091300-8eumrt4b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-fire-132
wandb: ‚≠êÔ∏è View project at https://wandb.ai/pcfg_pretrain/pretrain-
wandb: üöÄ View run at https://wandb.ai/pcfg_pretrain/pretrain-/runs/8eumrt4b
[rank: 0] Seed set to 42
{'data': {'batch_size': 1,
          'max_seq_length': -1,
          'num_workers': 4,
          'seed': 42,
          'tokenizer': None,
          'train_data_path': PosixPath('/data/cl/u/nsarkar/BabyLM_cleaned/train_set'),
          'val_data_path': PosixPath('/data/cl/u/nsarkar/BabyLM_cleaned/val_set')},
 'devices': 1,
 'eval': {'final_validation': True,
          'initial_validation': True,
          'interval': 100,
          'max_iters': 100,
          'max_new_tokens': None},
 'initial_checkpoint_dir': PosixPath('out/pretrain/custom-lm_75_1M_on_pcfg_25/final'),
 'logger_name': 'wandb',
 'model_config': {'bias': True,
                  'block_size': 1024,
                  'gelu_approximate': 'none',
                  'head_size': 64,
                  'hf_config': {},
                  'intermediate_size': 2048,
                  'lm_head_bias': False,
                  'mlp_class_name': 'LLaMAMLP',
                  'n_embd': 512,
                  'n_expert': 0,
                  'n_expert_per_token': 0,
                  'n_head': 8,
                  'n_layer': 6,
                  'n_query_groups': 4,
                  'name': '',
                  'norm_class_name': 'RMSNorm',
                  'norm_eps': 1e-05,
                  'padded_vocab_size': 50304,
                  'padding_multiple': 512,
                  'parallel_residual': True,
                  'rope_base': 10000,
                  'rope_condense_ratio': 1,
                  'rotary_percentage': 1.0,
                  'scale_embeddings': False,
                  'shared_attention_norm': False,
                  'vocab_size': 50254},
 'model_name': 'custom-lm_75_1M_pcfg_25_on_babylm',
 'optimizer': "{'class_path': 'torch.optim.AdamW', 'init_args': {'lr': 0.001, "
              "'weight_decay': 0.01, 'betas': [0.9, 0.95]}}",
 'out_dir': PosixPath('out/pretrain/custom-lm_75_1M_pcfg_25_on_babylm'),
 'precision': None,
 'resume': False,
 'seed': 42,
 'tokenizer_dir': PosixPath('/data/cl/u/nsarkar/litgpt/checkpoints/openai-community/gpt2'),
 'train': {'epochs': None,
           'global_batch_size': 32,
           'log_interval': 10,
           'lr_warmup_fraction': None,
           'lr_warmup_steps': 175,
           'max_norm': 1.0,
           'max_seq_length': None,
           'max_steps': None,
           'max_tokens': 63064386,
           'micro_batch_size': 32,
           'min_lr': 4e-05,
           'save_interval': 1000,
           'tie_embeddings': False}}
Time to instantiate model: 0.75 seconds.
Total parameters: 75,147,776
Validating ...
Measured TFLOPs: 10.94
Epoch 1 | iter 10 step 10 | loss train: 5.645, val: 5.631 | iter time: 69.55 ms (step) remaining time: 0:32:10
Epoch 1 | iter 20 step 20 | loss train: 5.422, val: 5.631 | iter time: 69.55 ms (step) remaining time: 0:17:05
Epoch 1 | iter 30 step 30 | loss train: 5.380, val: 5.631 | iter time: 69.69 ms (step) remaining time: 0:12:03
Epoch 1 | iter 40 step 40 | loss train: 5.291, val: 5.631 | iter time: 69.83 ms (step) remaining time: 0:09:32
Epoch 1 | iter 50 step 50 | loss train: 5.159, val: 5.631 | iter time: 69.54 ms (step) remaining time: 0:08:01
Epoch 1 | iter 60 step 60 | loss train: 5.113, val: 5.631 | iter time: 69.51 ms (step) remaining time: 0:07:00
Epoch 1 | iter 70 step 70 | loss train: 5.149, val: 5.631 | iter time: 69.85 ms (step) remaining time: 0:06:16
Epoch 1 | iter 80 step 80 | loss train: 5.040, val: 5.631 | iter time: 70.09 ms (step) remaining time: 0:05:43
Epoch 1 | iter 90 step 90 | loss train: 5.043, val: 5.631 | iter time: 69.52 ms (step) remaining time: 0:05:18
Epoch 1 | iter 100 step 100 | loss train: 4.942, val: 5.631 | iter time: 69.82 ms (step) remaining time: 0:04:57
Validating ...
iter 100: val loss 4.9774, val time: 2089.15 ms
Epoch 1 | iter 110 step 110 | loss train: 4.984, val: 4.977 | iter time: 69.59 ms (step) remaining time: 0:05:14
Epoch 1 | iter 120 step 120 | loss train: 4.917, val: 4.977 | iter time: 69.50 ms (step) remaining time: 0:04:57
Epoch 1 | iter 130 step 130 | loss train: 4.859, val: 4.977 | iter time: 70.22 ms (step) remaining time: 0:04:42
Epoch 1 | iter 140 step 140 | loss train: 4.775, val: 4.977 | iter time: 70.01 ms (step) remaining time: 0:04:29
Epoch 1 | iter 150 step 150 | loss train: 4.770, val: 4.977 | iter time: 69.66 ms (step) remaining time: 0:04:18
Epoch 1 | iter 160 step 160 | loss train: 4.833, val: 4.977 | iter time: 69.71 ms (step) remaining time: 0:04:08
Epoch 1 | iter 170 step 170 | loss train: 4.746, val: 4.977 | iter time: 70.46 ms (step) remaining time: 0:03:59
Epoch 1 | iter 180 step 180 | loss train: 4.746, val: 4.977 | iter time: 70.00 ms (step) remaining time: 0:03:51
Epoch 1 | iter 190 step 190 | loss train: 4.651, val: 4.977 | iter time: 69.73 ms (step) remaining time: 0:03:44
Epoch 1 | iter 200 step 200 | loss train: 4.670, val: 4.977 | iter time: 69.76 ms (step) remaining time: 0:03:38
Validating ...
iter 200: val loss 4.7090, val time: 2069.94 ms
Epoch 1 | iter 210 step 210 | loss train: 4.679, val: 4.709 | iter time: 69.87 ms (step) remaining time: 0:03:49
Epoch 1 | iter 220 step 220 | loss train: 4.664, val: 4.709 | iter time: 69.91 ms (step) remaining time: 0:03:42
Epoch 1 | iter 230 step 230 | loss train: 4.623, val: 4.709 | iter time: 70.09 ms (step) remaining time: 0:03:36
Epoch 1 | iter 240 step 240 | loss train: 4.657, val: 4.709 | iter time: 70.16 ms (step) remaining time: 0:03:31
Epoch 1 | iter 250 step 250 | loss train: 4.628, val: 4.709 | iter time: 69.86 ms (step) remaining time: 0:03:26
Epoch 1 | iter 260 step 260 | loss train: 4.604, val: 4.709 | iter time: 69.90 ms (step) remaining time: 0:03:21
Epoch 1 | iter 270 step 270 | loss train: 4.583, val: 4.709 | iter time: 69.87 ms (step) remaining time: 0:03:17
Epoch 1 | iter 280 step 280 | loss train: 4.609, val: 4.709 | iter time: 69.66 ms (step) remaining time: 0:03:13
Epoch 1 | iter 290 step 290 | loss train: 4.511, val: 4.709 | iter time: 69.35 ms (step) remaining time: 0:03:09
Epoch 1 | iter 300 step 300 | loss train: 4.509, val: 4.709 | iter time: 69.64 ms (step) remaining time: 0:03:05
Validating ...
iter 300: val loss 4.5631, val time: 2093.12 ms
Epoch 1 | iter 310 step 310 | loss train: 4.513, val: 4.563 | iter time: 69.20 ms (step) remaining time: 0:03:13
Epoch 1 | iter 320 step 320 | loss train: 4.538, val: 4.563 | iter time: 69.17 ms (step) remaining time: 0:03:09
Epoch 2 | iter 330 step 330 | loss train: 4.332, val: 4.563 | iter time: 69.64 ms (step) remaining time: 0:03:06
Epoch 2 | iter 340 step 340 | loss train: 4.347, val: 4.563 | iter time: 69.61 ms (step) remaining time: 0:03:03
Epoch 2 | iter 350 step 350 | loss train: 4.374, val: 4.563 | iter time: 69.39 ms (step) remaining time: 0:02:59
Epoch 2 | iter 360 step 360 | loss train: 4.409, val: 4.563 | iter time: 69.62 ms (step) remaining time: 0:02:56
Epoch 2 | iter 370 step 370 | loss train: 4.368, val: 4.563 | iter time: 69.68 ms (step) remaining time: 0:02:53
Epoch 2 | iter 380 step 380 | loss train: 4.373, val: 4.563 | iter time: 69.62 ms (step) remaining time: 0:02:50
Epoch 2 | iter 390 step 390 | loss train: 4.284, val: 4.563 | iter time: 69.32 ms (step) remaining time: 0:02:48
Epoch 2 | iter 400 step 400 | loss train: 4.377, val: 4.563 | iter time: 69.31 ms (step) remaining time: 0:02:45
Validating ...
iter 400: val loss 4.4827, val time: 2075.73 ms
Epoch 2 | iter 410 step 410 | loss train: 4.336, val: 4.483 | iter time: 69.30 ms (step) remaining time: 0:02:50
Epoch 2 | iter 420 step 420 | loss train: 4.393, val: 4.483 | iter time: 69.16 ms (step) remaining time: 0:02:47
Epoch 2 | iter 430 step 430 | loss train: 4.336, val: 4.483 | iter time: 69.51 ms (step) remaining time: 0:02:45
Epoch 2 | iter 440 step 440 | loss train: 4.348, val: 4.483 | iter time: 69.53 ms (step) remaining time: 0:02:42
Epoch 2 | iter 450 step 450 | loss train: 4.396, val: 4.483 | iter time: 69.52 ms (step) remaining time: 0:02:40
Epoch 2 | iter 460 step 460 | loss train: 4.301, val: 4.483 | iter time: 69.36 ms (step) remaining time: 0:02:38
Epoch 2 | iter 470 step 470 | loss train: 4.357, val: 4.483 | iter time: 69.38 ms (step) remaining time: 0:02:35
Epoch 2 | iter 480 step 480 | loss train: 4.360, val: 4.483 | iter time: 69.55 ms (step) remaining time: 0:02:33
Epoch 2 | iter 490 step 490 | loss train: 4.270, val: 4.483 | iter time: 69.26 ms (step) remaining time: 0:02:31
Epoch 2 | iter 500 step 500 | loss train: 4.344, val: 4.483 | iter time: 69.18 ms (step) remaining time: 0:02:29
Validating ...
iter 500: val loss 4.4149, val time: 2084.67 ms
Epoch 2 | iter 510 step 510 | loss train: 4.318, val: 4.415 | iter time: 69.59 ms (step) remaining time: 0:02:33
Epoch 2 | iter 520 step 520 | loss train: 4.272, val: 4.415 | iter time: 69.42 ms (step) remaining time: 0:02:30
Epoch 2 | iter 530 step 530 | loss train: 4.345, val: 4.415 | iter time: 69.98 ms (step) remaining time: 0:02:28
Epoch 2 | iter 540 step 540 | loss train: 4.241, val: 4.415 | iter time: 69.58 ms (step) remaining time: 0:02:26
Epoch 2 | iter 550 step 550 | loss train: 4.295, val: 4.415 | iter time: 69.44 ms (step) remaining time: 0:02:24
Epoch 2 | iter 560 step 560 | loss train: 4.304, val: 4.415 | iter time: 69.21 ms (step) remaining time: 0:02:22
Epoch 2 | iter 570 step 570 | loss train: 4.282, val: 4.415 | iter time: 69.63 ms (step) remaining time: 0:02:20
Epoch 2 | iter 580 step 580 | loss train: 4.283, val: 4.415 | iter time: 69.61 ms (step) remaining time: 0:02:19
Epoch 2 | iter 590 step 590 | loss train: 4.301, val: 4.415 | iter time: 69.49 ms (step) remaining time: 0:02:17
Epoch 2 | iter 600 step 600 | loss train: 4.263, val: 4.415 | iter time: 69.26 ms (step) remaining time: 0:02:15
Validating ...
iter 600: val loss 4.3592, val time: 2084.19 ms
Epoch 2 | iter 610 step 610 | loss train: 4.288, val: 4.359 | iter time: 69.32 ms (step) remaining time: 0:02:18
Epoch 2 | iter 620 step 620 | loss train: 4.267, val: 4.359 | iter time: 69.26 ms (step) remaining time: 0:02:16
Epoch 2 | iter 630 step 630 | loss train: 4.254, val: 4.359 | iter time: 69.49 ms (step) remaining time: 0:02:14
Epoch 2 | iter 640 step 640 | loss train: 4.313, val: 4.359 | iter time: 69.86 ms (step) remaining time: 0:02:12
Epoch 3 | iter 650 step 650 | loss train: 4.046, val: 4.359 | iter time: 454.85 ms (step) remaining time: 0:02:12
Epoch 3 | iter 660 step 660 | loss train: 3.968, val: 4.359 | iter time: 69.21 ms (step) remaining time: 0:02:10
Epoch 3 | iter 670 step 670 | loss train: 3.969, val: 4.359 | iter time: 69.63 ms (step) remaining time: 0:02:08
Epoch 3 | iter 680 step 680 | loss train: 4.054, val: 4.359 | iter time: 69.72 ms (step) remaining time: 0:02:07
Epoch 3 | iter 690 step 690 | loss train: 4.011, val: 4.359 | iter time: 69.26 ms (step) remaining time: 0:02:05
Epoch 3 | iter 700 step 700 | loss train: 4.018, val: 4.359 | iter time: 69.25 ms (step) remaining time: 0:02:03
Validating ...
iter 700: val loss 4.3390, val time: 2073.28 ms
Epoch 3 | iter 710 step 710 | loss train: 4.057, val: 4.339 | iter time: 69.25 ms (step) remaining time: 0:02:05
Epoch 3 | iter 720 step 720 | loss train: 4.044, val: 4.339 | iter time: 69.14 ms (step) remaining time: 0:02:04
Epoch 3 | iter 730 step 730 | loss train: 4.060, val: 4.339 | iter time: 69.47 ms (step) remaining time: 0:02:02
Epoch 3 | iter 740 step 740 | loss train: 4.020, val: 4.339 | iter time: 69.72 ms (step) remaining time: 0:02:01
Epoch 3 | iter 750 step 750 | loss train: 4.082, val: 4.339 | iter time: 69.50 ms (step) remaining time: 0:01:59
Epoch 3 | iter 760 step 760 | loss train: 4.043, val: 4.339 | iter time: 69.33 ms (step) remaining time: 0:01:58
Epoch 3 | iter 770 step 770 | loss train: 4.066, val: 4.339 | iter time: 69.81 ms (step) remaining time: 0:01:56
Epoch 3 | iter 780 step 780 | loss train: 4.038, val: 4.339 | iter time: 69.85 ms (step) remaining time: 0:01:55
Epoch 3 | iter 790 step 790 | loss train: 4.068, val: 4.339 | iter time: 69.50 ms (step) remaining time: 0:01:53
Epoch 3 | iter 800 step 800 | loss train: 4.107, val: 4.339 | iter time: 69.45 ms (step) remaining time: 0:01:52
Validating ...
iter 800: val loss 4.3067, val time: 2098.90 ms
Epoch 3 | iter 810 step 810 | loss train: 4.024, val: 4.307 | iter time: 69.35 ms (step) remaining time: 0:01:53
Epoch 3 | iter 820 step 820 | loss train: 4.097, val: 4.307 | iter time: 69.19 ms (step) remaining time: 0:01:52
Epoch 3 | iter 830 step 830 | loss train: 4.028, val: 4.307 | iter time: 69.52 ms (step) remaining time: 0:01:50
Epoch 3 | iter 840 step 840 | loss train: 4.078, val: 4.307 | iter time: 69.57 ms (step) remaining time: 0:01:49
Epoch 3 | iter 850 step 850 | loss train: 4.071, val: 4.307 | iter time: 69.37 ms (step) remaining time: 0:01:47
Epoch 3 | iter 860 step 860 | loss train: 4.085, val: 4.307 | iter time: 69.32 ms (step) remaining time: 0:01:46
Epoch 3 | iter 870 step 870 | loss train: 4.013, val: 4.307 | iter time: 69.67 ms (step) remaining time: 0:01:45
Epoch 3 | iter 880 step 880 | loss train: 4.083, val: 4.307 | iter time: 69.73 ms (step) remaining time: 0:01:43
Epoch 3 | iter 890 step 890 | loss train: 4.005, val: 4.307 | iter time: 69.28 ms (step) remaining time: 0:01:42
Epoch 3 | iter 900 step 900 | loss train: 4.035, val: 4.307 | iter time: 69.31 ms (step) remaining time: 0:01:41
Validating ...
iter 900: val loss 4.2730, val time: 2101.56 ms
Epoch 3 | iter 910 step 910 | loss train: 4.104, val: 4.273 | iter time: 69.36 ms (step) remaining time: 0:01:42
Epoch 3 | iter 920 step 920 | loss train: 4.036, val: 4.273 | iter time: 69.25 ms (step) remaining time: 0:01:40
Epoch 3 | iter 930 step 930 | loss train: 4.085, val: 4.273 | iter time: 69.52 ms (step) remaining time: 0:01:39
Epoch 3 | iter 940 step 940 | loss train: 4.024, val: 4.273 | iter time: 69.54 ms (step) remaining time: 0:01:38
Epoch 3 | iter 950 step 950 | loss train: 4.068, val: 4.273 | iter time: 69.29 ms (step) remaining time: 0:01:36
Epoch 3 | iter 960 step 960 | loss train: 4.001, val: 4.273 | iter time: 69.19 ms (step) remaining time: 0:01:35
Epoch 4 | iter 970 step 970 | loss train: 3.799, val: 4.273 | iter time: 69.72 ms (step) remaining time: 0:01:34
Epoch 4 | iter 980 step 980 | loss train: 3.799, val: 4.273 | iter time: 69.70 ms (step) remaining time: 0:01:33
Epoch 4 | iter 990 step 990 | loss train: 3.791, val: 4.273 | iter time: 69.49 ms (step) remaining time: 0:01:31
Epoch 4 | iter 1000 step 1000 | loss train: 3.801, val: 4.273 | iter time: 69.20 ms (step) remaining time: 0:01:30
Validating ...
iter 1000: val loss 4.2892, val time: 2087.96 ms
Saving checkpoint to 'out/pretrain/custom-lm_75_1M_pcfg_25_on_babylm/step-00001000/lit_model.pth'
Epoch 4 | iter 1010 step 1010 | loss train: 3.808, val: 4.289 | iter time: 69.37 ms (step) remaining time: 0:01:52
Epoch 4 | iter 1020 step 1020 | loss train: 3.788, val: 4.289 | iter time: 69.24 ms (step) remaining time: 0:01:51
Epoch 4 | iter 1030 step 1030 | loss train: 3.745, val: 4.289 | iter time: 69.51 ms (step) remaining time: 0:01:49
Epoch 4 | iter 1040 step 1040 | loss train: 3.758, val: 4.289 | iter time: 69.60 ms (step) remaining time: 0:01:47
Epoch 4 | iter 1050 step 1050 | loss train: 3.784, val: 4.289 | iter time: 69.43 ms (step) remaining time: 0:01:45
Epoch 4 | iter 1060 step 1060 | loss train: 3.819, val: 4.289 | iter time: 69.24 ms (step) remaining time: 0:01:44
Epoch 4 | iter 1070 step 1070 | loss train: 3.793, val: 4.289 | iter time: 69.58 ms (step) remaining time: 0:01:42
Epoch 4 | iter 1080 step 1080 | loss train: 3.788, val: 4.289 | iter time: 69.77 ms (step) remaining time: 0:01:41
Epoch 4 | iter 1090 step 1090 | loss train: 3.835, val: 4.289 | iter time: 69.38 ms (step) remaining time: 0:01:39
Epoch 4 | iter 1100 step 1100 | loss train: 3.765, val: 4.289 | iter time: 69.67 ms (step) remaining time: 0:01:37
Validating ...
iter 1100: val loss 4.2751, val time: 2090.23 ms
Epoch 4 | iter 1110 step 1110 | loss train: 3.788, val: 4.275 | iter time: 69.45 ms (step) remaining time: 0:01:37
Epoch 4 | iter 1120 step 1120 | loss train: 3.798, val: 4.275 | iter time: 69.44 ms (step) remaining time: 0:01:36
Epoch 4 | iter 1130 step 1130 | loss train: 3.712, val: 4.275 | iter time: 69.69 ms (step) remaining time: 0:01:34
Epoch 4 | iter 1140 step 1140 | loss train: 3.821, val: 4.275 | iter time: 69.74 ms (step) remaining time: 0:01:33
Epoch 4 | iter 1150 step 1150 | loss train: 3.866, val: 4.275 | iter time: 69.43 ms (step) remaining time: 0:01:31
Epoch 4 | iter 1160 step 1160 | loss train: 3.841, val: 4.275 | iter time: 69.27 ms (step) remaining time: 0:01:30
Epoch 4 | iter 1170 step 1170 | loss train: 3.780, val: 4.275 | iter time: 69.59 ms (step) remaining time: 0:01:28
Epoch 4 | iter 1180 step 1180 | loss train: 3.811, val: 4.275 | iter time: 69.66 ms (step) remaining time: 0:01:27
Epoch 4 | iter 1190 step 1190 | loss train: 3.789, val: 4.275 | iter time: 69.31 ms (step) remaining time: 0:01:25
Epoch 4 | iter 1200 step 1200 | loss train: 3.839, val: 4.275 | iter time: 70.05 ms (step) remaining time: 0:01:24
Validating ...
iter 1200: val loss 4.2548, val time: 2083.34 ms
Epoch 4 | iter 1210 step 1210 | loss train: 3.796, val: 4.255 | iter time: 69.42 ms (step) remaining time: 0:01:24
Epoch 4 | iter 1220 step 1220 | loss train: 3.855, val: 4.255 | iter time: 69.12 ms (step) remaining time: 0:01:22
Epoch 4 | iter 1230 step 1230 | loss train: 3.785, val: 4.255 | iter time: 69.45 ms (step) remaining time: 0:01:21
Epoch 4 | iter 1240 step 1240 | loss train: 3.787, val: 4.255 | iter time: 69.65 ms (step) remaining time: 0:01:19
Epoch 4 | iter 1250 step 1250 | loss train: 3.768, val: 4.255 | iter time: 69.51 ms (step) remaining time: 0:01:18
Epoch 4 | iter 1260 step 1260 | loss train: 3.865, val: 4.255 | iter time: 69.22 ms (step) remaining time: 0:01:16
Epoch 4 | iter 1270 step 1270 | loss train: 3.881, val: 4.255 | iter time: 70.02 ms (step) remaining time: 0:01:15
Epoch 4 | iter 1280 step 1280 | loss train: 3.831, val: 4.255 | iter time: 69.74 ms (step) remaining time: 0:01:14
Epoch 5 | iter 1290 step 1290 | loss train: 3.543, val: 4.255 | iter time: 69.39 ms (step) remaining time: 0:01:12
Epoch 5 | iter 1300 step 1300 | loss train: 3.575, val: 4.255 | iter time: 69.53 ms (step) remaining time: 0:01:11
Validating ...
iter 1300: val loss 4.2807, val time: 2088.18 ms
Epoch 5 | iter 1310 step 1310 | loss train: 3.547, val: 4.281 | iter time: 69.37 ms (step) remaining time: 0:01:11
Epoch 5 | iter 1320 step 1320 | loss train: 3.548, val: 4.281 | iter time: 69.30 ms (step) remaining time: 0:01:09
Epoch 5 | iter 1330 step 1330 | loss train: 3.555, val: 4.281 | iter time: 69.47 ms (step) remaining time: 0:01:08
Epoch 5 | iter 1340 step 1340 | loss train: 3.574, val: 4.281 | iter time: 69.77 ms (step) remaining time: 0:01:06
Epoch 5 | iter 1350 step 1350 | loss train: 3.560, val: 4.281 | iter time: 69.42 ms (step) remaining time: 0:01:05
Epoch 5 | iter 1360 step 1360 | loss train: 3.609, val: 4.281 | iter time: 69.17 ms (step) remaining time: 0:01:04
Epoch 5 | iter 1370 step 1370 | loss train: 3.575, val: 4.281 | iter time: 69.53 ms (step) remaining time: 0:01:02
Epoch 5 | iter 1380 step 1380 | loss train: 3.560, val: 4.281 | iter time: 69.66 ms (step) remaining time: 0:01:01
Epoch 5 | iter 1390 step 1390 | loss train: 3.604, val: 4.281 | iter time: 69.18 ms (step) remaining time: 0:01:00
Epoch 5 | iter 1400 step 1400 | loss train: 3.597, val: 4.281 | iter time: 69.37 ms (step) remaining time: 0:00:59
Validating ...
iter 1400: val loss 4.2923, val time: 2080.82 ms
Epoch 5 | iter 1410 step 1410 | loss train: 3.626, val: 4.292 | iter time: 69.33 ms (step) remaining time: 0:00:58
Epoch 5 | iter 1420 step 1420 | loss train: 3.574, val: 4.292 | iter time: 69.26 ms (step) remaining time: 0:00:57
Epoch 5 | iter 1430 step 1430 | loss train: 3.626, val: 4.292 | iter time: 69.64 ms (step) remaining time: 0:00:55
Epoch 5 | iter 1440 step 1440 | loss train: 3.609, val: 4.292 | iter time: 69.72 ms (step) remaining time: 0:00:54
Epoch 5 | iter 1450 step 1450 | loss train: 3.623, val: 4.292 | iter time: 69.21 ms (step) remaining time: 0:00:53
Epoch 5 | iter 1460 step 1460 | loss train: 3.579, val: 4.292 | iter time: 69.14 ms (step) remaining time: 0:00:52
Epoch 5 | iter 1470 step 1470 | loss train: 3.511, val: 4.292 | iter time: 69.60 ms (step) remaining time: 0:00:50
Epoch 5 | iter 1480 step 1480 | loss train: 3.549, val: 4.292 | iter time: 69.93 ms (step) remaining time: 0:00:49
Epoch 5 | iter 1490 step 1490 | loss train: 3.608, val: 4.292 | iter time: 69.38 ms (step) remaining time: 0:00:48
Epoch 5 | iter 1500 step 1500 | loss train: 3.567, val: 4.292 | iter time: 69.35 ms (step) remaining time: 0:00:47
Validating ...
iter 1500: val loss 4.2860, val time: 2086.08 ms
Epoch 5 | iter 1510 step 1510 | loss train: 3.611, val: 4.286 | iter time: 69.47 ms (step) remaining time: 0:00:46
Epoch 5 | iter 1520 step 1520 | loss train: 3.587, val: 4.286 | iter time: 69.52 ms (step) remaining time: 0:00:45
Epoch 5 | iter 1530 step 1530 | loss train: 3.557, val: 4.286 | iter time: 69.54 ms (step) remaining time: 0:00:44
Epoch 5 | iter 1540 step 1540 | loss train: 3.614, val: 4.286 | iter time: 69.58 ms (step) remaining time: 0:00:42
Epoch 5 | iter 1550 step 1550 | loss train: 3.587, val: 4.286 | iter time: 69.39 ms (step) remaining time: 0:00:41
Epoch 5 | iter 1560 step 1560 | loss train: 3.580, val: 4.286 | iter time: 69.28 ms (step) remaining time: 0:00:40
Epoch 5 | iter 1570 step 1570 | loss train: 3.582, val: 4.286 | iter time: 69.60 ms (step) remaining time: 0:00:39
Epoch 5 | iter 1580 step 1580 | loss train: 3.608, val: 4.286 | iter time: 69.68 ms (step) remaining time: 0:00:37
Epoch 5 | iter 1590 step 1590 | loss train: 3.614, val: 4.286 | iter time: 69.33 ms (step) remaining time: 0:00:36
Epoch 5 | iter 1600 step 1600 | loss train: 3.548, val: 4.286 | iter time: 69.45 ms (step) remaining time: 0:00:35
Validating ...
iter 1600: val loss 4.2773, val time: 2093.78 ms
Epoch 6 | iter 1610 step 1610 | loss train: 3.391, val: 4.277 | iter time: 69.56 ms (step) remaining time: 0:00:34
Epoch 6 | iter 1620 step 1620 | loss train: 3.448, val: 4.277 | iter time: 69.66 ms (step) remaining time: 0:00:33
Epoch 6 | iter 1630 step 1630 | loss train: 3.422, val: 4.277 | iter time: 71.65 ms (step) remaining time: 0:00:32
Epoch 6 | iter 1640 step 1640 | loss train: 3.471, val: 4.277 | iter time: 70.25 ms (step) remaining time: 0:00:31
Epoch 6 | iter 1650 step 1650 | loss train: 3.425, val: 4.277 | iter time: 69.34 ms (step) remaining time: 0:00:30
Epoch 6 | iter 1660 step 1660 | loss train: 3.362, val: 4.277 | iter time: 69.69 ms (step) remaining time: 0:00:28
Epoch 6 | iter 1670 step 1670 | loss train: 3.386, val: 4.277 | iter time: 69.45 ms (step) remaining time: 0:00:27
Epoch 6 | iter 1680 step 1680 | loss train: 3.458, val: 4.277 | iter time: 69.62 ms (step) remaining time: 0:00:26
Epoch 6 | iter 1690 step 1690 | loss train: 3.429, val: 4.277 | iter time: 69.54 ms (step) remaining time: 0:00:25
Epoch 6 | iter 1700 step 1700 | loss train: 3.415, val: 4.277 | iter time: 70.02 ms (step) remaining time: 0:00:24
Validating ...
iter 1700: val loss 4.3224, val time: 2086.01 ms
Epoch 6 | iter 1710 step 1710 | loss train: 3.439, val: 4.322 | iter time: 69.38 ms (step) remaining time: 0:00:23
Epoch 6 | iter 1720 step 1720 | loss train: 3.453, val: 4.322 | iter time: 69.21 ms (step) remaining time: 0:00:22
Epoch 6 | iter 1730 step 1730 | loss train: 3.398, val: 4.322 | iter time: 69.66 ms (step) remaining time: 0:00:21
Epoch 6 | iter 1740 step 1740 | loss train: 3.438, val: 4.322 | iter time: 69.58 ms (step) remaining time: 0:00:20
Epoch 6 | iter 1750 step 1750 | loss train: 3.436, val: 4.322 | iter time: 69.46 ms (step) remaining time: 0:00:18
Epoch 6 | iter 1760 step 1760 | loss train: 3.428, val: 4.322 | iter time: 69.30 ms (step) remaining time: 0:00:17
Epoch 6 | iter 1770 step 1770 | loss train: 3.452, val: 4.322 | iter time: 69.69 ms (step) remaining time: 0:00:16
Epoch 6 | iter 1780 step 1780 | loss train: 3.429, val: 4.322 | iter time: 69.52 ms (step) remaining time: 0:00:15
Epoch 6 | iter 1790 step 1790 | loss train: 3.495, val: 4.322 | iter time: 69.21 ms (step) remaining time: 0:00:14
Epoch 6 | iter 1800 step 1800 | loss train: 3.406, val: 4.322 | iter time: 69.32 ms (step) remaining time: 0:00:13
Validating ...
iter 1800: val loss 4.3242, val time: 2084.64 ms
Epoch 6 | iter 1810 step 1810 | loss train: 3.469, val: 4.324 | iter time: 69.60 ms (step) remaining time: 0:00:12
Epoch 6 | iter 1820 step 1820 | loss train: 3.440, val: 4.324 | iter time: 69.44 ms (step) remaining time: 0:00:11
Epoch 6 | iter 1830 step 1830 | loss train: 3.491, val: 4.324 | iter time: 69.60 ms (step) remaining time: 0:00:10
Epoch 6 | iter 1840 step 1840 | loss train: 3.416, val: 4.324 | iter time: 69.63 ms (step) remaining time: 0:00:09
Epoch 6 | iter 1850 step 1850 | loss train: 3.460, val: 4.324 | iter time: 69.30 ms (step) remaining time: 0:00:07
Epoch 6 | iter 1860 step 1860 | loss train: 3.494, val: 4.324 | iter time: 69.22 ms (step) remaining time: 0:00:06
Epoch 6 | iter 1870 step 1870 | loss train: 3.410, val: 4.324 | iter time: 69.55 ms (step) remaining time: 0:00:05
Epoch 6 | iter 1880 step 1880 | loss train: 3.421, val: 4.324 | iter time: 69.66 ms (step) remaining time: 0:00:04
Epoch 6 | iter 1890 step 1890 | loss train: 3.457, val: 4.324 | iter time: 69.38 ms (step) remaining time: 0:00:03
Epoch 6 | iter 1900 step 1900 | loss train: 3.451, val: 4.324 | iter time: 69.23 ms (step) remaining time: 0:00:02
Validating ...
iter 1900: val loss 4.3247, val time: 2087.87 ms
Epoch 6 | iter 1910 step 1910 | loss train: 3.427, val: 4.325 | iter time: 69.41 ms (step) remaining time: 0:00:01
Epoch 6 | iter 1920 step 1920 | loss train: 3.446, val: 4.325 | iter time: 69.21 ms (step) remaining time: 0:00:00
Validating ...
Final evaluation | val loss: 4.325 | val ppl: 75.548
Saving checkpoint to 'out/pretrain/custom-lm_75_1M_pcfg_25_on_babylm/final/lit_model.pth'
Training time: 245.63s
Memory used: 24.98 GB
wandb: - 0.021 MB of 0.021 MB uploadedwandb: \ 0.055 MB of 0.055 MB uploadedwandb: 
wandb: Run history:
wandb:                batches ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: device/batches_per_sec ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ
wandb:   device/flops_per_sec ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ
wandb:   device/items_per_sec ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ
wandb:             device/mfu ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ
wandb: device/samples_per_sec ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ
wandb:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                   iter ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:              iter_time ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñá‚ñÇ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÅ‚ñÜ‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ
wandb:          learning_rate ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                lengths ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         remaining_time ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                samples ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   time ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                 tokens ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:           total_tokens ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:    trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:               val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                val_ppl ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                batches 1920
wandb: device/batches_per_sec 8.25651
wandb:   device/flops_per_sec 90320246131081.98
wandb:   device/items_per_sec 270549.19195
wandb:             device/mfu 0.09129
wandb: device/samples_per_sec 264.2082
wandb:                  epoch 5
wandb:                   iter 1920
wandb:              iter_time 0.06921
wandb:          learning_rate 4e-05
wandb:                lengths 62914560
wandb:                   loss 3.44565
wandb:         remaining_time 0.43076
wandb:                samples 61440
wandb:                   step 1920
wandb:                   time 206.76596
wandb:                 tokens 62914560
wandb:           total_tokens 62914560
wandb:    trainer/global_step 1924
wandb:               val_loss 4.32477
wandb:                val_ppl 75.54792
wandb: 
wandb: üöÄ View run denim-fire-132 at: https://wandb.ai/pcfg_pretrain/pretrain-/runs/8eumrt4b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/pcfg_pretrain/pretrain-
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240708_091300-8eumrt4b/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
