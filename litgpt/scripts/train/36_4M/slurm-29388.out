/data/cl/u/nsarkar/miniconda/envs/litgpt/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data/cl/u/nsarkar/miniconda/envs/litgpt/bin/litgpt  ...
Using bfloat16 Automatic Mixed Precision (AMP)
wandb: Currently logged in as: nandan-sarkar (pcfg_pretrain). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.2
wandb: Run data is saved locally in ./wandb/run-20240704_113436-qgionf1f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-puddle-116
wandb: ‚≠êÔ∏è View project at https://wandb.ai/pcfg_pretrain/pretrain-
wandb: üöÄ View run at https://wandb.ai/pcfg_pretrain/pretrain-/runs/qgionf1f
[rank: 0] Seed set to 42
{'data': {'batch_size': 1,
          'max_seq_length': -1,
          'num_workers': 4,
          'seed': 42,
          'tokenizer': None,
          'train_data_path': PosixPath('/data/cl/u/nsarkar/BabyLM_cleaned/train_set'),
          'val_data_path': PosixPath('/data/cl/u/nsarkar/BabyLM_cleaned/val_set')},
 'devices': 1,
 'eval': {'final_validation': True,
          'initial_validation': True,
          'interval': 100,
          'max_iters': 100,
          'max_new_tokens': None},
 'initial_checkpoint_dir': PosixPath('out/pretrain/custom-lm_36_4M_on_pcfg_10/final'),
 'logger_name': 'wandb',
 'model_config': {'bias': True,
                  'block_size': 1024,
                  'gelu_approximate': 'none',
                  'head_size': 32,
                  'hf_config': {},
                  'intermediate_size': 2048,
                  'lm_head_bias': False,
                  'mlp_class_name': 'LLaMAMLP',
                  'n_embd': 256,
                  'n_expert': 0,
                  'n_expert_per_token': 0,
                  'n_head': 8,
                  'n_layer': 6,
                  'n_query_groups': 4,
                  'name': '',
                  'norm_class_name': 'RMSNorm',
                  'norm_eps': 1e-05,
                  'padded_vocab_size': 50304,
                  'padding_multiple': 512,
                  'parallel_residual': True,
                  'rope_base': 10000,
                  'rope_condense_ratio': 1,
                  'rotary_percentage': 1.0,
                  'scale_embeddings': False,
                  'shared_attention_norm': False,
                  'vocab_size': 50254},
 'model_name': 'custom_model_36_4M_pcfg_10_on_babylm',
 'optimizer': "{'class_path': 'torch.optim.AdamW', 'init_args': {'lr': 0.001, "
              "'weight_decay': 0.01, 'betas': [0.9, 0.95]}}",
 'out_dir': PosixPath('out/pretrain/custom_model_36_4M_pcfg_10_on_babylm'),
 'precision': None,
 'resume': False,
 'seed': 42,
 'tokenizer_dir': PosixPath('/data/cl/u/nsarkar/litgpt/checkpoints/openai-community/gpt2'),
 'train': {'epochs': None,
           'global_batch_size': 16,
           'log_interval': 10,
           'lr_warmup_fraction': None,
           'lr_warmup_steps': 350,
           'max_norm': 1.0,
           'max_seq_length': None,
           'max_steps': None,
           'max_tokens': 63064386,
           'micro_batch_size': 16,
           'min_lr': 4e-05,
           'save_interval': 1000,
           'tie_embeddings': False}}
Time to instantiate model: 0.18 seconds.
Total parameters: 36,406,528
Validating ...
Measured TFLOPs: 2.62
Epoch 1 | iter 10 step 10 | loss train: 5.511, val: 5.686 | iter time: 112.48 ms (step) remaining time: 1:29:15
Epoch 1 | iter 20 step 20 | loss train: 5.655, val: 5.686 | iter time: 112.75 ms (step) remaining time: 0:48:00
Epoch 1 | iter 30 step 30 | loss train: 5.543, val: 5.686 | iter time: 112.25 ms (step) remaining time: 0:34:15
Epoch 1 | iter 40 step 40 | loss train: 5.442, val: 5.686 | iter time: 112.26 ms (step) remaining time: 0:27:21
Epoch 1 | iter 50 step 50 | loss train: 5.422, val: 5.686 | iter time: 112.87 ms (step) remaining time: 0:23:13
Epoch 1 | iter 60 step 60 | loss train: 5.422, val: 5.686 | iter time: 113.00 ms (step) remaining time: 0:20:27
Epoch 1 | iter 70 step 70 | loss train: 5.364, val: 5.686 | iter time: 112.43 ms (step) remaining time: 0:18:28
Epoch 1 | iter 80 step 80 | loss train: 5.343, val: 5.686 | iter time: 112.26 ms (step) remaining time: 0:16:58
Epoch 1 | iter 90 step 90 | loss train: 5.316, val: 5.686 | iter time: 112.18 ms (step) remaining time: 0:15:48
Epoch 1 | iter 100 step 100 | loss train: 5.204, val: 5.686 | iter time: 112.53 ms (step) remaining time: 0:14:52
Validating ...
iter 100: val loss 5.2541, val time: 4071.32 ms
Epoch 1 | iter 110 step 110 | loss train: 5.298, val: 5.254 | iter time: 112.05 ms (step) remaining time: 0:16:25
Epoch 1 | iter 120 step 120 | loss train: 5.060, val: 5.254 | iter time: 111.96 ms (step) remaining time: 0:15:34
Epoch 1 | iter 130 step 130 | loss train: 5.193, val: 5.254 | iter time: 112.65 ms (step) remaining time: 0:14:51
Epoch 1 | iter 140 step 140 | loss train: 5.114, val: 5.254 | iter time: 112.68 ms (step) remaining time: 0:14:14
Epoch 1 | iter 150 step 150 | loss train: 5.083, val: 5.254 | iter time: 112.35 ms (step) remaining time: 0:13:42
Epoch 1 | iter 160 step 160 | loss train: 5.084, val: 5.254 | iter time: 112.20 ms (step) remaining time: 0:13:14
Epoch 1 | iter 170 step 170 | loss train: 5.142, val: 5.254 | iter time: 112.66 ms (step) remaining time: 0:12:49
Epoch 1 | iter 180 step 180 | loss train: 4.982, val: 5.254 | iter time: 113.08 ms (step) remaining time: 0:12:26
Epoch 1 | iter 190 step 190 | loss train: 5.083, val: 5.254 | iter time: 112.59 ms (step) remaining time: 0:12:06
Epoch 1 | iter 200 step 200 | loss train: 4.993, val: 5.254 | iter time: 112.10 ms (step) remaining time: 0:11:48
Validating ...
iter 200: val loss 5.0343, val time: 4101.66 ms
Epoch 1 | iter 210 step 210 | loss train: 4.990, val: 5.034 | iter time: 112.32 ms (step) remaining time: 0:12:43
Epoch 1 | iter 220 step 220 | loss train: 5.054, val: 5.034 | iter time: 112.46 ms (step) remaining time: 0:12:24
Epoch 1 | iter 230 step 230 | loss train: 4.940, val: 5.034 | iter time: 112.18 ms (step) remaining time: 0:12:07
Epoch 1 | iter 240 step 240 | loss train: 4.954, val: 5.034 | iter time: 112.58 ms (step) remaining time: 0:11:51
Epoch 1 | iter 250 step 250 | loss train: 4.953, val: 5.034 | iter time: 112.94 ms (step) remaining time: 0:11:37
Epoch 1 | iter 260 step 260 | loss train: 4.897, val: 5.034 | iter time: 113.37 ms (step) remaining time: 0:11:23
Epoch 1 | iter 270 step 270 | loss train: 4.880, val: 5.034 | iter time: 113.39 ms (step) remaining time: 0:11:11
Epoch 1 | iter 280 step 280 | loss train: 4.798, val: 5.034 | iter time: 112.28 ms (step) remaining time: 0:10:59
Epoch 1 | iter 290 step 290 | loss train: 4.828, val: 5.034 | iter time: 112.71 ms (step) remaining time: 0:10:48
Epoch 1 | iter 300 step 300 | loss train: 4.886, val: 5.034 | iter time: 112.76 ms (step) remaining time: 0:10:37
Validating ...
iter 300: val loss 4.9027, val time: 4094.87 ms
Epoch 1 | iter 310 step 310 | loss train: 4.791, val: 4.903 | iter time: 113.18 ms (step) remaining time: 0:11:14
Epoch 1 | iter 320 step 320 | loss train: 4.849, val: 4.903 | iter time: 113.37 ms (step) remaining time: 0:11:04
Epoch 1 | iter 330 step 330 | loss train: 4.886, val: 4.903 | iter time: 113.52 ms (step) remaining time: 0:10:53
Epoch 1 | iter 340 step 340 | loss train: 4.819, val: 4.903 | iter time: 113.38 ms (step) remaining time: 0:10:44
Epoch 1 | iter 350 step 350 | loss train: 4.794, val: 4.903 | iter time: 113.36 ms (step) remaining time: 0:10:35
Epoch 1 | iter 360 step 360 | loss train: 4.882, val: 4.903 | iter time: 113.12 ms (step) remaining time: 0:10:26
Epoch 1 | iter 370 step 370 | loss train: 4.845, val: 4.903 | iter time: 113.01 ms (step) remaining time: 0:10:17
Epoch 1 | iter 380 step 380 | loss train: 4.738, val: 4.903 | iter time: 112.81 ms (step) remaining time: 0:10:09
Epoch 1 | iter 390 step 390 | loss train: 4.767, val: 4.903 | iter time: 112.69 ms (step) remaining time: 0:10:02
Epoch 1 | iter 400 step 400 | loss train: 4.779, val: 4.903 | iter time: 112.59 ms (step) remaining time: 0:09:55
Validating ...
iter 400: val loss 4.8065, val time: 4087.05 ms
Epoch 1 | iter 410 step 410 | loss train: 4.719, val: 4.806 | iter time: 113.37 ms (step) remaining time: 0:10:22
Epoch 1 | iter 420 step 420 | loss train: 4.728, val: 4.806 | iter time: 113.44 ms (step) remaining time: 0:10:14
Epoch 1 | iter 430 step 430 | loss train: 4.742, val: 4.806 | iter time: 112.41 ms (step) remaining time: 0:10:07
Epoch 1 | iter 440 step 440 | loss train: 4.675, val: 4.806 | iter time: 113.50 ms (step) remaining time: 0:10:00
Epoch 1 | iter 450 step 450 | loss train: 4.711, val: 4.806 | iter time: 113.79 ms (step) remaining time: 0:09:53
Epoch 1 | iter 460 step 460 | loss train: 4.750, val: 4.806 | iter time: 113.45 ms (step) remaining time: 0:09:47
Epoch 1 | iter 470 step 470 | loss train: 4.750, val: 4.806 | iter time: 113.28 ms (step) remaining time: 0:09:40
Epoch 1 | iter 480 step 480 | loss train: 4.798, val: 4.806 | iter time: 113.73 ms (step) remaining time: 0:09:34
Epoch 1 | iter 490 step 490 | loss train: 4.732, val: 4.806 | iter time: 113.69 ms (step) remaining time: 0:09:28
Epoch 1 | iter 500 step 500 | loss train: 4.827, val: 4.806 | iter time: 113.65 ms (step) remaining time: 0:09:23
Validating ...
iter 500: val loss 4.7301, val time: 4080.88 ms
Epoch 1 | iter 510 step 510 | loss train: 4.713, val: 4.730 | iter time: 113.39 ms (step) remaining time: 0:09:44
Epoch 1 | iter 520 step 520 | loss train: 4.709, val: 4.730 | iter time: 113.38 ms (step) remaining time: 0:09:38
Epoch 1 | iter 530 step 530 | loss train: 4.710, val: 4.730 | iter time: 113.39 ms (step) remaining time: 0:09:32
Epoch 1 | iter 540 step 540 | loss train: 4.622, val: 4.730 | iter time: 113.20 ms (step) remaining time: 0:09:27
Epoch 1 | iter 550 step 550 | loss train: 4.712, val: 4.730 | iter time: 113.67 ms (step) remaining time: 0:09:21
Epoch 1 | iter 560 step 560 | loss train: 4.694, val: 4.730 | iter time: 112.96 ms (step) remaining time: 0:09:16
Epoch 1 | iter 570 step 570 | loss train: 4.628, val: 4.730 | iter time: 112.96 ms (step) remaining time: 0:09:11
Epoch 1 | iter 580 step 580 | loss train: 4.606, val: 4.730 | iter time: 112.73 ms (step) remaining time: 0:09:06
Epoch 1 | iter 590 step 590 | loss train: 4.649, val: 4.730 | iter time: 112.80 ms (step) remaining time: 0:09:01
Epoch 1 | iter 600 step 600 | loss train: 4.622, val: 4.730 | iter time: 112.73 ms (step) remaining time: 0:08:57
Validating ...
iter 600: val loss 4.6707, val time: 4085.92 ms
Epoch 1 | iter 610 step 610 | loss train: 4.584, val: 4.671 | iter time: 113.61 ms (step) remaining time: 0:09:14
Epoch 1 | iter 620 step 620 | loss train: 4.620, val: 4.671 | iter time: 113.46 ms (step) remaining time: 0:09:09
Epoch 1 | iter 630 step 630 | loss train: 4.578, val: 4.671 | iter time: 113.32 ms (step) remaining time: 0:09:04
Epoch 1 | iter 640 step 640 | loss train: 4.682, val: 4.671 | iter time: 113.87 ms (step) remaining time: 0:08:59
Epoch 2 | iter 650 step 650 | loss train: 4.470, val: 4.671 | iter time: 112.99 ms (step) remaining time: 0:08:56
Epoch 2 | iter 660 step 660 | loss train: 4.407, val: 4.671 | iter time: 112.89 ms (step) remaining time: 0:08:51
Epoch 2 | iter 670 step 670 | loss train: 4.550, val: 4.671 | iter time: 112.91 ms (step) remaining time: 0:08:47
Epoch 2 | iter 680 step 680 | loss train: 4.478, val: 4.671 | iter time: 112.80 ms (step) remaining time: 0:08:43
Epoch 2 | iter 690 step 690 | loss train: 4.528, val: 4.671 | iter time: 112.83 ms (step) remaining time: 0:08:38
Epoch 2 | iter 700 step 700 | loss train: 4.541, val: 4.671 | iter time: 113.24 ms (step) remaining time: 0:08:34
Validating ...
iter 700: val loss 4.6341, val time: 4089.57 ms
Epoch 2 | iter 710 step 710 | loss train: 4.515, val: 4.634 | iter time: 112.83 ms (step) remaining time: 0:08:48
Epoch 2 | iter 720 step 720 | loss train: 4.506, val: 4.634 | iter time: 113.00 ms (step) remaining time: 0:08:44
Epoch 2 | iter 730 step 730 | loss train: 4.500, val: 4.634 | iter time: 113.07 ms (step) remaining time: 0:08:40
Epoch 2 | iter 740 step 740 | loss train: 4.549, val: 4.634 | iter time: 112.82 ms (step) remaining time: 0:08:36
Epoch 2 | iter 750 step 750 | loss train: 4.466, val: 4.634 | iter time: 113.11 ms (step) remaining time: 0:08:32
Epoch 2 | iter 760 step 760 | loss train: 4.500, val: 4.634 | iter time: 113.42 ms (step) remaining time: 0:08:28
Epoch 2 | iter 770 step 770 | loss train: 4.491, val: 4.634 | iter time: 113.41 ms (step) remaining time: 0:08:24
Epoch 2 | iter 780 step 780 | loss train: 4.458, val: 4.634 | iter time: 113.65 ms (step) remaining time: 0:08:21
Epoch 2 | iter 790 step 790 | loss train: 4.542, val: 4.634 | iter time: 112.91 ms (step) remaining time: 0:08:17
Epoch 2 | iter 800 step 800 | loss train: 4.483, val: 4.634 | iter time: 112.73 ms (step) remaining time: 0:08:13
Validating ...
iter 800: val loss 4.5976, val time: 4090.72 ms
Epoch 2 | iter 810 step 810 | loss train: 4.577, val: 4.598 | iter time: 113.15 ms (step) remaining time: 0:08:25
Epoch 2 | iter 820 step 820 | loss train: 4.489, val: 4.598 | iter time: 112.57 ms (step) remaining time: 0:08:21
Epoch 2 | iter 830 step 830 | loss train: 4.497, val: 4.598 | iter time: 112.61 ms (step) remaining time: 0:08:18
Epoch 2 | iter 840 step 840 | loss train: 4.515, val: 4.598 | iter time: 112.83 ms (step) remaining time: 0:08:14
Epoch 2 | iter 850 step 850 | loss train: 4.508, val: 4.598 | iter time: 112.63 ms (step) remaining time: 0:08:10
Epoch 2 | iter 860 step 860 | loss train: 4.537, val: 4.598 | iter time: 112.63 ms (step) remaining time: 0:08:07
Epoch 2 | iter 870 step 870 | loss train: 4.498, val: 4.598 | iter time: 112.70 ms (step) remaining time: 0:08:03
Epoch 2 | iter 880 step 880 | loss train: 4.504, val: 4.598 | iter time: 112.67 ms (step) remaining time: 0:08:00
Epoch 2 | iter 890 step 890 | loss train: 4.455, val: 4.598 | iter time: 112.79 ms (step) remaining time: 0:07:57
Epoch 2 | iter 900 step 900 | loss train: 4.512, val: 4.598 | iter time: 112.73 ms (step) remaining time: 0:07:53
Validating ...
iter 900: val loss 4.5628, val time: 4109.33 ms
Epoch 2 | iter 910 step 910 | loss train: 4.434, val: 4.563 | iter time: 112.90 ms (step) remaining time: 0:08:03
Epoch 2 | iter 920 step 920 | loss train: 4.456, val: 4.563 | iter time: 112.76 ms (step) remaining time: 0:08:00
Epoch 2 | iter 930 step 930 | loss train: 4.543, val: 4.563 | iter time: 112.98 ms (step) remaining time: 0:07:57
Epoch 2 | iter 940 step 940 | loss train: 4.563, val: 4.563 | iter time: 113.04 ms (step) remaining time: 0:07:53
Epoch 2 | iter 950 step 950 | loss train: 4.340, val: 4.563 | iter time: 113.17 ms (step) remaining time: 0:07:50
Epoch 2 | iter 960 step 960 | loss train: 4.423, val: 4.563 | iter time: 112.87 ms (step) remaining time: 0:07:47
Epoch 2 | iter 970 step 970 | loss train: 4.373, val: 4.563 | iter time: 112.73 ms (step) remaining time: 0:07:44
Epoch 2 | iter 980 step 980 | loss train: 4.469, val: 4.563 | iter time: 112.82 ms (step) remaining time: 0:07:41
Epoch 2 | iter 990 step 990 | loss train: 4.392, val: 4.563 | iter time: 112.70 ms (step) remaining time: 0:07:38
Epoch 2 | iter 1000 step 1000 | loss train: 4.538, val: 4.563 | iter time: 112.82 ms (step) remaining time: 0:07:35
Validating ...
iter 1000: val loss 4.5408, val time: 4077.08 ms
Saving checkpoint to 'out/pretrain/custom_model_36_4M_pcfg_10_on_babylm/step-00001000/lit_model.pth'
Epoch 2 | iter 1010 step 1010 | loss train: 4.476, val: 4.541 | iter time: 112.63 ms (step) remaining time: 0:07:58
Epoch 2 | iter 1020 step 1020 | loss train: 4.458, val: 4.541 | iter time: 112.61 ms (step) remaining time: 0:07:55
Epoch 2 | iter 1030 step 1030 | loss train: 4.376, val: 4.541 | iter time: 112.66 ms (step) remaining time: 0:07:52
Epoch 2 | iter 1040 step 1040 | loss train: 4.361, val: 4.541 | iter time: 112.60 ms (step) remaining time: 0:07:48
Epoch 2 | iter 1050 step 1050 | loss train: 4.385, val: 4.541 | iter time: 112.69 ms (step) remaining time: 0:07:45
Epoch 2 | iter 1060 step 1060 | loss train: 4.521, val: 4.541 | iter time: 112.59 ms (step) remaining time: 0:07:42
Epoch 2 | iter 1070 step 1070 | loss train: 4.355, val: 4.541 | iter time: 112.74 ms (step) remaining time: 0:07:39
Epoch 2 | iter 1080 step 1080 | loss train: 4.388, val: 4.541 | iter time: 112.66 ms (step) remaining time: 0:07:36
Epoch 2 | iter 1090 step 1090 | loss train: 4.387, val: 4.541 | iter time: 112.64 ms (step) remaining time: 0:07:33
Epoch 2 | iter 1100 step 1100 | loss train: 4.438, val: 4.541 | iter time: 112.73 ms (step) remaining time: 0:07:30
Validating ...
iter 1100: val loss 4.5057, val time: 4109.74 ms
Epoch 2 | iter 1110 step 1110 | loss train: 4.543, val: 4.506 | iter time: 113.15 ms (step) remaining time: 0:07:37
Epoch 2 | iter 1120 step 1120 | loss train: 4.421, val: 4.506 | iter time: 113.40 ms (step) remaining time: 0:07:34
Epoch 2 | iter 1130 step 1130 | loss train: 4.412, val: 4.506 | iter time: 113.30 ms (step) remaining time: 0:07:31
Epoch 2 | iter 1140 step 1140 | loss train: 4.425, val: 4.506 | iter time: 113.39 ms (step) remaining time: 0:07:28
Epoch 2 | iter 1150 step 1150 | loss train: 4.475, val: 4.506 | iter time: 113.27 ms (step) remaining time: 0:07:25
Epoch 2 | iter 1160 step 1160 | loss train: 4.410, val: 4.506 | iter time: 113.47 ms (step) remaining time: 0:07:22
Epoch 2 | iter 1170 step 1170 | loss train: 4.372, val: 4.506 | iter time: 113.34 ms (step) remaining time: 0:07:19
Epoch 2 | iter 1180 step 1180 | loss train: 4.425, val: 4.506 | iter time: 113.12 ms (step) remaining time: 0:07:16
Epoch 2 | iter 1190 step 1190 | loss train: 4.431, val: 4.506 | iter time: 113.05 ms (step) remaining time: 0:07:13
Epoch 2 | iter 1200 step 1200 | loss train: 4.362, val: 4.506 | iter time: 113.09 ms (step) remaining time: 0:07:11
Validating ...
iter 1200: val loss 4.4833, val time: 4078.46 ms
Epoch 2 | iter 1210 step 1210 | loss train: 4.411, val: 4.483 | iter time: 112.64 ms (step) remaining time: 0:07:17
Epoch 2 | iter 1220 step 1220 | loss train: 4.468, val: 4.483 | iter time: 112.67 ms (step) remaining time: 0:07:14
Epoch 2 | iter 1230 step 1230 | loss train: 4.402, val: 4.483 | iter time: 112.71 ms (step) remaining time: 0:07:11
Epoch 2 | iter 1240 step 1240 | loss train: 4.418, val: 4.483 | iter time: 112.90 ms (step) remaining time: 0:07:08
Epoch 2 | iter 1250 step 1250 | loss train: 4.398, val: 4.483 | iter time: 112.78 ms (step) remaining time: 0:07:06
Epoch 2 | iter 1260 step 1260 | loss train: 4.434, val: 4.483 | iter time: 112.75 ms (step) remaining time: 0:07:03
Epoch 2 | iter 1270 step 1270 | loss train: 4.411, val: 4.483 | iter time: 112.74 ms (step) remaining time: 0:07:00
Epoch 2 | iter 1280 step 1280 | loss train: 4.436, val: 4.483 | iter time: 112.63 ms (step) remaining time: 0:06:57
Epoch 3 | iter 1290 step 1290 | loss train: 4.259, val: 4.483 | iter time: 113.12 ms (step) remaining time: 0:06:55
Epoch 3 | iter 1300 step 1300 | loss train: 4.229, val: 4.483 | iter time: 113.62 ms (step) remaining time: 0:06:52
Validating ...
iter 1300: val loss 4.4653, val time: 4083.62 ms
Epoch 3 | iter 1310 step 1310 | loss train: 4.206, val: 4.465 | iter time: 112.65 ms (step) remaining time: 0:06:58
Epoch 3 | iter 1320 step 1320 | loss train: 4.183, val: 4.465 | iter time: 112.56 ms (step) remaining time: 0:06:55
Epoch 3 | iter 1330 step 1330 | loss train: 4.361, val: 4.465 | iter time: 112.70 ms (step) remaining time: 0:06:52
Epoch 3 | iter 1340 step 1340 | loss train: 4.140, val: 4.465 | iter time: 112.88 ms (step) remaining time: 0:06:50
Epoch 3 | iter 1350 step 1350 | loss train: 4.157, val: 4.465 | iter time: 112.69 ms (step) remaining time: 0:06:47
Epoch 3 | iter 1360 step 1360 | loss train: 4.227, val: 4.465 | iter time: 112.97 ms (step) remaining time: 0:06:44
Epoch 3 | iter 1370 step 1370 | loss train: 4.279, val: 4.465 | iter time: 112.88 ms (step) remaining time: 0:06:42
Epoch 3 | iter 1380 step 1380 | loss train: 4.187, val: 4.465 | iter time: 113.03 ms (step) remaining time: 0:06:39
Epoch 3 | iter 1390 step 1390 | loss train: 4.312, val: 4.465 | iter time: 113.10 ms (step) remaining time: 0:06:37
Epoch 3 | iter 1400 step 1400 | loss train: 4.228, val: 4.465 | iter time: 113.12 ms (step) remaining time: 0:06:34
Validating ...
iter 1400: val loss 4.4530, val time: 4086.39 ms
Epoch 3 | iter 1410 step 1410 | loss train: 4.317, val: 4.453 | iter time: 112.90 ms (step) remaining time: 0:06:39
Epoch 3 | iter 1420 step 1420 | loss train: 4.285, val: 4.453 | iter time: 112.69 ms (step) remaining time: 0:06:36
Epoch 3 | iter 1430 step 1430 | loss train: 4.327, val: 4.453 | iter time: 112.74 ms (step) remaining time: 0:06:34
Epoch 3 | iter 1440 step 1440 | loss train: 4.272, val: 4.453 | iter time: 112.82 ms (step) remaining time: 0:06:31
Epoch 3 | iter 1450 step 1450 | loss train: 4.206, val: 4.453 | iter time: 113.41 ms (step) remaining time: 0:06:29
Epoch 3 | iter 1460 step 1460 | loss train: 4.341, val: 4.453 | iter time: 112.70 ms (step) remaining time: 0:06:26
Epoch 3 | iter 1470 step 1470 | loss train: 4.276, val: 4.453 | iter time: 112.68 ms (step) remaining time: 0:06:24
Epoch 3 | iter 1480 step 1480 | loss train: 4.259, val: 4.453 | iter time: 112.87 ms (step) remaining time: 0:06:21
Epoch 3 | iter 1490 step 1490 | loss train: 4.326, val: 4.453 | iter time: 112.81 ms (step) remaining time: 0:06:19
Epoch 3 | iter 1500 step 1500 | loss train: 4.236, val: 4.453 | iter time: 112.91 ms (step) remaining time: 0:06:17
Validating ...
iter 1500: val loss 4.4361, val time: 4091.80 ms
Epoch 3 | iter 1510 step 1510 | loss train: 4.233, val: 4.436 | iter time: 112.84 ms (step) remaining time: 0:06:21
Epoch 3 | iter 1520 step 1520 | loss train: 4.283, val: 4.436 | iter time: 113.03 ms (step) remaining time: 0:06:18
Epoch 3 | iter 1530 step 1530 | loss train: 4.223, val: 4.436 | iter time: 112.78 ms (step) remaining time: 0:06:16
Epoch 3 | iter 1540 step 1540 | loss train: 4.310, val: 4.436 | iter time: 112.63 ms (step) remaining time: 0:06:13
Epoch 3 | iter 1550 step 1550 | loss train: 4.268, val: 4.436 | iter time: 113.40 ms (step) remaining time: 0:06:11
Epoch 3 | iter 1560 step 1560 | loss train: 4.240, val: 4.436 | iter time: 113.12 ms (step) remaining time: 0:06:08
Epoch 3 | iter 1570 step 1570 | loss train: 4.335, val: 4.436 | iter time: 113.07 ms (step) remaining time: 0:06:06
Epoch 3 | iter 1580 step 1580 | loss train: 4.230, val: 4.436 | iter time: 113.65 ms (step) remaining time: 0:06:04
Epoch 3 | iter 1590 step 1590 | loss train: 4.205, val: 4.436 | iter time: 113.42 ms (step) remaining time: 0:06:01
Epoch 3 | iter 1600 step 1600 | loss train: 4.293, val: 4.436 | iter time: 113.46 ms (step) remaining time: 0:05:59
Validating ...
iter 1600: val loss 4.4164, val time: 4100.52 ms
Epoch 3 | iter 1610 step 1610 | loss train: 4.154, val: 4.416 | iter time: 112.92 ms (step) remaining time: 0:06:03
Epoch 3 | iter 1620 step 1620 | loss train: 4.178, val: 4.416 | iter time: 112.86 ms (step) remaining time: 0:06:00
Epoch 3 | iter 1630 step 1630 | loss train: 4.281, val: 4.416 | iter time: 112.76 ms (step) remaining time: 0:05:58
Epoch 3 | iter 1640 step 1640 | loss train: 4.279, val: 4.416 | iter time: 112.67 ms (step) remaining time: 0:05:56
Epoch 3 | iter 1650 step 1650 | loss train: 4.391, val: 4.416 | iter time: 112.91 ms (step) remaining time: 0:05:53
Epoch 3 | iter 1660 step 1660 | loss train: 4.250, val: 4.416 | iter time: 112.82 ms (step) remaining time: 0:05:51
Epoch 3 | iter 1670 step 1670 | loss train: 4.275, val: 4.416 | iter time: 112.85 ms (step) remaining time: 0:05:49
Epoch 3 | iter 1680 step 1680 | loss train: 4.326, val: 4.416 | iter time: 112.86 ms (step) remaining time: 0:05:47
Epoch 3 | iter 1690 step 1690 | loss train: 4.244, val: 4.416 | iter time: 112.76 ms (step) remaining time: 0:05:44
Epoch 3 | iter 1700 step 1700 | loss train: 4.162, val: 4.416 | iter time: 113.08 ms (step) remaining time: 0:05:42
Validating ...
iter 1700: val loss 4.3986, val time: 4109.29 ms
Epoch 3 | iter 1710 step 1710 | loss train: 4.293, val: 4.399 | iter time: 112.94 ms (step) remaining time: 0:05:45
Epoch 3 | iter 1720 step 1720 | loss train: 4.269, val: 4.399 | iter time: 112.64 ms (step) remaining time: 0:05:43
Epoch 3 | iter 1730 step 1730 | loss train: 4.324, val: 4.399 | iter time: 112.63 ms (step) remaining time: 0:05:41
Epoch 3 | iter 1740 step 1740 | loss train: 4.189, val: 4.399 | iter time: 112.73 ms (step) remaining time: 0:05:38
Epoch 3 | iter 1750 step 1750 | loss train: 4.176, val: 4.399 | iter time: 112.59 ms (step) remaining time: 0:05:36
Epoch 3 | iter 1760 step 1760 | loss train: 4.324, val: 4.399 | iter time: 113.05 ms (step) remaining time: 0:05:34
Epoch 3 | iter 1770 step 1770 | loss train: 4.192, val: 4.399 | iter time: 113.21 ms (step) remaining time: 0:05:32
Epoch 3 | iter 1780 step 1780 | loss train: 4.183, val: 4.399 | iter time: 113.57 ms (step) remaining time: 0:05:30
Epoch 3 | iter 1790 step 1790 | loss train: 4.215, val: 4.399 | iter time: 113.52 ms (step) remaining time: 0:05:27
Epoch 3 | iter 1800 step 1800 | loss train: 4.200, val: 4.399 | iter time: 113.12 ms (step) remaining time: 0:05:25
Validating ...
iter 1800: val loss 4.3857, val time: 4080.73 ms
Epoch 3 | iter 1810 step 1810 | loss train: 4.183, val: 4.386 | iter time: 112.69 ms (step) remaining time: 0:05:28
Epoch 3 | iter 1820 step 1820 | loss train: 4.257, val: 4.386 | iter time: 112.99 ms (step) remaining time: 0:05:25
Epoch 3 | iter 1830 step 1830 | loss train: 4.168, val: 4.386 | iter time: 112.73 ms (step) remaining time: 0:05:23
Epoch 3 | iter 1840 step 1840 | loss train: 4.152, val: 4.386 | iter time: 112.88 ms (step) remaining time: 0:05:21
Epoch 3 | iter 1850 step 1850 | loss train: 4.177, val: 4.386 | iter time: 112.70 ms (step) remaining time: 0:05:19
Epoch 3 | iter 1860 step 1860 | loss train: 4.218, val: 4.386 | iter time: 113.08 ms (step) remaining time: 0:05:17
Epoch 3 | iter 1870 step 1870 | loss train: 4.239, val: 4.386 | iter time: 113.23 ms (step) remaining time: 0:05:15
Epoch 3 | iter 1880 step 1880 | loss train: 4.254, val: 4.386 | iter time: 113.16 ms (step) remaining time: 0:05:13
Epoch 3 | iter 1890 step 1890 | loss train: 4.244, val: 4.386 | iter time: 113.62 ms (step) remaining time: 0:05:11
Epoch 3 | iter 1900 step 1900 | loss train: 4.280, val: 4.386 | iter time: 113.82 ms (step) remaining time: 0:05:08
Validating ...
iter 1900: val loss 4.3677, val time: 4102.01 ms
Epoch 3 | iter 1910 step 1910 | loss train: 4.234, val: 4.368 | iter time: 112.82 ms (step) remaining time: 0:05:11
Epoch 3 | iter 1920 step 1920 | loss train: 4.208, val: 4.368 | iter time: 112.70 ms (step) remaining time: 0:05:08
Epoch 4 | iter 1930 step 1930 | loss train: 4.013, val: 4.368 | iter time: 113.17 ms (step) remaining time: 0:05:07
Epoch 4 | iter 1940 step 1940 | loss train: 4.119, val: 4.368 | iter time: 113.06 ms (step) remaining time: 0:05:04
Epoch 4 | iter 1950 step 1950 | loss train: 3.998, val: 4.368 | iter time: 113.71 ms (step) remaining time: 0:05:02
Epoch 4 | iter 1960 step 1960 | loss train: 4.068, val: 4.368 | iter time: 112.61 ms (step) remaining time: 0:05:00
Epoch 4 | iter 1970 step 1970 | loss train: 4.186, val: 4.368 | iter time: 113.34 ms (step) remaining time: 0:04:58
Epoch 4 | iter 1980 step 1980 | loss train: 4.056, val: 4.368 | iter time: 112.87 ms (step) remaining time: 0:04:56
Epoch 4 | iter 1990 step 1990 | loss train: 4.083, val: 4.368 | iter time: 112.79 ms (step) remaining time: 0:04:54
Epoch 4 | iter 2000 step 2000 | loss train: 4.122, val: 4.368 | iter time: 111.85 ms (step) remaining time: 0:04:52
Validating ...
iter 2000: val loss 4.3789, val time: 4090.30 ms
Saving checkpoint to 'out/pretrain/custom_model_36_4M_pcfg_10_on_babylm/step-00002000/lit_model.pth'
Epoch 4 | iter 2010 step 2010 | loss train: 4.043, val: 4.379 | iter time: 112.63 ms (step) remaining time: 0:04:58
Epoch 4 | iter 2020 step 2020 | loss train: 4.088, val: 4.379 | iter time: 112.89 ms (step) remaining time: 0:04:56
Epoch 4 | iter 2030 step 2030 | loss train: 4.095, val: 4.379 | iter time: 112.74 ms (step) remaining time: 0:04:54
Epoch 4 | iter 2040 step 2040 | loss train: 4.007, val: 4.379 | iter time: 112.66 ms (step) remaining time: 0:04:52
Epoch 4 | iter 2050 step 2050 | loss train: 4.160, val: 4.379 | iter time: 112.68 ms (step) remaining time: 0:04:50
Epoch 4 | iter 2060 step 2060 | loss train: 4.007, val: 4.379 | iter time: 112.76 ms (step) remaining time: 0:04:48
Epoch 4 | iter 2070 step 2070 | loss train: 4.126, val: 4.379 | iter time: 112.58 ms (step) remaining time: 0:04:46
Epoch 4 | iter 2080 step 2080 | loss train: 4.032, val: 4.379 | iter time: 112.39 ms (step) remaining time: 0:04:44
Epoch 4 | iter 2090 step 2090 | loss train: 4.053, val: 4.379 | iter time: 113.14 ms (step) remaining time: 0:04:42
Epoch 4 | iter 2100 step 2100 | loss train: 4.076, val: 4.379 | iter time: 112.70 ms (step) remaining time: 0:04:40
Validating ...
iter 2100: val loss 4.3678, val time: 4091.50 ms
Epoch 4 | iter 2110 step 2110 | loss train: 3.985, val: 4.368 | iter time: 112.61 ms (step) remaining time: 0:04:41
Epoch 4 | iter 2120 step 2120 | loss train: 4.105, val: 4.368 | iter time: 112.72 ms (step) remaining time: 0:04:39
Epoch 4 | iter 2130 step 2130 | loss train: 4.046, val: 4.368 | iter time: 112.77 ms (step) remaining time: 0:04:37
Epoch 4 | iter 2140 step 2140 | loss train: 4.044, val: 4.368 | iter time: 112.69 ms (step) remaining time: 0:04:35
Epoch 4 | iter 2150 step 2150 | loss train: 4.031, val: 4.368 | iter time: 112.92 ms (step) remaining time: 0:04:33
Epoch 4 | iter 2160 step 2160 | loss train: 4.043, val: 4.368 | iter time: 112.88 ms (step) remaining time: 0:04:31
Epoch 4 | iter 2170 step 2170 | loss train: 4.116, val: 4.368 | iter time: 112.99 ms (step) remaining time: 0:04:29
Epoch 4 | iter 2180 step 2180 | loss train: 4.055, val: 4.368 | iter time: 113.30 ms (step) remaining time: 0:04:27
Epoch 4 | iter 2190 step 2190 | loss train: 4.137, val: 4.368 | iter time: 113.73 ms (step) remaining time: 0:04:25
Epoch 4 | iter 2200 step 2200 | loss train: 4.029, val: 4.368 | iter time: 113.22 ms (step) remaining time: 0:04:23
Validating ...
iter 2200: val loss 4.3560, val time: 4099.61 ms
Epoch 4 | iter 2210 step 2210 | loss train: 4.106, val: 4.356 | iter time: 112.67 ms (step) remaining time: 0:04:24
Epoch 4 | iter 2220 step 2220 | loss train: 4.061, val: 4.356 | iter time: 112.69 ms (step) remaining time: 0:04:22
Epoch 4 | iter 2230 step 2230 | loss train: 4.092, val: 4.356 | iter time: 112.89 ms (step) remaining time: 0:04:20
Epoch 4 | iter 2240 step 2240 | loss train: 4.088, val: 4.356 | iter time: 112.96 ms (step) remaining time: 0:04:18
Epoch 4 | iter 2250 step 2250 | loss train: 4.036, val: 4.356 | iter time: 112.80 ms (step) remaining time: 0:04:16
Epoch 4 | iter 2260 step 2260 | loss train: 3.999, val: 4.356 | iter time: 112.75 ms (step) remaining time: 0:04:14
Epoch 4 | iter 2270 step 2270 | loss train: 4.146, val: 4.356 | iter time: 112.82 ms (step) remaining time: 0:04:12
Epoch 4 | iter 2280 step 2280 | loss train: 4.066, val: 4.356 | iter time: 112.74 ms (step) remaining time: 0:04:10
Epoch 4 | iter 2290 step 2290 | loss train: 4.046, val: 4.356 | iter time: 112.72 ms (step) remaining time: 0:04:08
Epoch 4 | iter 2300 step 2300 | loss train: 4.192, val: 4.356 | iter time: 113.67 ms (step) remaining time: 0:04:07
Validating ...
iter 2300: val loss 4.3446, val time: 4105.11 ms
Epoch 4 | iter 2310 step 2310 | loss train: 4.145, val: 4.345 | iter time: 113.08 ms (step) remaining time: 0:04:07
Epoch 4 | iter 2320 step 2320 | loss train: 4.059, val: 4.345 | iter time: 112.91 ms (step) remaining time: 0:04:05
Epoch 4 | iter 2330 step 2330 | loss train: 4.038, val: 4.345 | iter time: 112.71 ms (step) remaining time: 0:04:03
Epoch 4 | iter 2340 step 2340 | loss train: 4.079, val: 4.345 | iter time: 112.91 ms (step) remaining time: 0:04:02
Epoch 4 | iter 2350 step 2350 | loss train: 4.087, val: 4.345 | iter time: 112.79 ms (step) remaining time: 0:04:00
Epoch 4 | iter 2360 step 2360 | loss train: 4.070, val: 4.345 | iter time: 112.91 ms (step) remaining time: 0:03:58
Epoch 4 | iter 2370 step 2370 | loss train: 4.095, val: 4.345 | iter time: 112.93 ms (step) remaining time: 0:03:56
Epoch 4 | iter 2380 step 2380 | loss train: 4.065, val: 4.345 | iter time: 112.87 ms (step) remaining time: 0:03:54
Epoch 4 | iter 2390 step 2390 | loss train: 4.055, val: 4.345 | iter time: 112.91 ms (step) remaining time: 0:03:52
Epoch 4 | iter 2400 step 2400 | loss train: 4.099, val: 4.345 | iter time: 112.92 ms (step) remaining time: 0:03:50
Validating ...
iter 2400: val loss 4.3319, val time: 4077.48 ms
Epoch 4 | iter 2410 step 2410 | loss train: 4.143, val: 4.332 | iter time: 112.81 ms (step) remaining time: 0:03:51
Epoch 4 | iter 2420 step 2420 | loss train: 3.956, val: 4.332 | iter time: 112.73 ms (step) remaining time: 0:03:49
Epoch 4 | iter 2430 step 2430 | loss train: 4.030, val: 4.332 | iter time: 112.69 ms (step) remaining time: 0:03:47
Epoch 4 | iter 2440 step 2440 | loss train: 4.127, val: 4.332 | iter time: 112.67 ms (step) remaining time: 0:03:45
Epoch 4 | iter 2450 step 2450 | loss train: 4.079, val: 4.332 | iter time: 112.84 ms (step) remaining time: 0:03:43
Epoch 4 | iter 2460 step 2460 | loss train: 4.018, val: 4.332 | iter time: 112.76 ms (step) remaining time: 0:03:41
Epoch 4 | iter 2470 step 2470 | loss train: 4.084, val: 4.332 | iter time: 112.18 ms (step) remaining time: 0:03:39
Epoch 4 | iter 2480 step 2480 | loss train: 4.046, val: 4.332 | iter time: 113.02 ms (step) remaining time: 0:03:37
Epoch 4 | iter 2490 step 2490 | loss train: 4.064, val: 4.332 | iter time: 112.90 ms (step) remaining time: 0:03:36
Epoch 4 | iter 2500 step 2500 | loss train: 4.049, val: 4.332 | iter time: 112.82 ms (step) remaining time: 0:03:34
Validating ...
iter 2500: val loss 4.3214, val time: 4090.71 ms
Epoch 4 | iter 2510 step 2510 | loss train: 3.967, val: 4.321 | iter time: 112.84 ms (step) remaining time: 0:03:34
Epoch 4 | iter 2520 step 2520 | loss train: 4.186, val: 4.321 | iter time: 112.81 ms (step) remaining time: 0:03:32
Epoch 4 | iter 2530 step 2530 | loss train: 4.015, val: 4.321 | iter time: 112.64 ms (step) remaining time: 0:03:30
Epoch 4 | iter 2540 step 2540 | loss train: 4.140, val: 4.321 | iter time: 112.75 ms (step) remaining time: 0:03:28
Epoch 4 | iter 2550 step 2550 | loss train: 4.055, val: 4.321 | iter time: 112.81 ms (step) remaining time: 0:03:27
Epoch 4 | iter 2560 step 2560 | loss train: 4.163, val: 4.321 | iter time: 112.77 ms (step) remaining time: 0:03:25
Epoch 5 | iter 2570 step 2570 | loss train: 3.944, val: 4.321 | iter time: 113.58 ms (step) remaining time: 0:03:23
Epoch 5 | iter 2580 step 2580 | loss train: 3.864, val: 4.321 | iter time: 113.22 ms (step) remaining time: 0:03:21
Epoch 5 | iter 2590 step 2590 | loss train: 3.875, val: 4.321 | iter time: 113.10 ms (step) remaining time: 0:03:19
Epoch 5 | iter 2600 step 2600 | loss train: 3.911, val: 4.321 | iter time: 112.99 ms (step) remaining time: 0:03:18
Validating ...
iter 2600: val loss 4.3349, val time: 4079.45 ms
Epoch 5 | iter 2610 step 2610 | loss train: 3.956, val: 4.335 | iter time: 112.87 ms (step) remaining time: 0:03:18
Epoch 5 | iter 2620 step 2620 | loss train: 3.895, val: 4.335 | iter time: 112.89 ms (step) remaining time: 0:03:16
Epoch 5 | iter 2630 step 2630 | loss train: 3.821, val: 4.335 | iter time: 112.92 ms (step) remaining time: 0:03:14
Epoch 5 | iter 2640 step 2640 | loss train: 3.988, val: 4.335 | iter time: 112.97 ms (step) remaining time: 0:03:12
Epoch 5 | iter 2650 step 2650 | loss train: 3.936, val: 4.335 | iter time: 113.14 ms (step) remaining time: 0:03:10
Epoch 5 | iter 2660 step 2660 | loss train: 3.881, val: 4.335 | iter time: 113.25 ms (step) remaining time: 0:03:09
Epoch 5 | iter 2670 step 2670 | loss train: 3.966, val: 4.335 | iter time: 113.64 ms (step) remaining time: 0:03:07
Epoch 5 | iter 2680 step 2680 | loss train: 3.866, val: 4.335 | iter time: 113.73 ms (step) remaining time: 0:03:05
Epoch 5 | iter 2690 step 2690 | loss train: 3.895, val: 4.335 | iter time: 113.54 ms (step) remaining time: 0:03:03
Epoch 5 | iter 2700 step 2700 | loss train: 3.932, val: 4.335 | iter time: 113.38 ms (step) remaining time: 0:03:01
Validating ...
iter 2700: val loss 4.3365, val time: 4119.22 ms
Epoch 5 | iter 2710 step 2710 | loss train: 3.911, val: 4.336 | iter time: 113.04 ms (step) remaining time: 0:03:01
Epoch 5 | iter 2720 step 2720 | loss train: 3.899, val: 4.336 | iter time: 112.81 ms (step) remaining time: 0:03:00
Epoch 5 | iter 2730 step 2730 | loss train: 3.918, val: 4.336 | iter time: 112.99 ms (step) remaining time: 0:02:58
Epoch 5 | iter 2740 step 2740 | loss train: 3.930, val: 4.336 | iter time: 112.94 ms (step) remaining time: 0:02:56
Epoch 5 | iter 2750 step 2750 | loss train: 3.875, val: 4.336 | iter time: 112.98 ms (step) remaining time: 0:02:54
Epoch 5 | iter 2760 step 2760 | loss train: 3.877, val: 4.336 | iter time: 113.17 ms (step) remaining time: 0:02:52
Epoch 5 | iter 2770 step 2770 | loss train: 3.926, val: 4.336 | iter time: 113.35 ms (step) remaining time: 0:02:51
Epoch 5 | iter 2780 step 2780 | loss train: 3.895, val: 4.336 | iter time: 113.46 ms (step) remaining time: 0:02:49
Epoch 5 | iter 2790 step 2790 | loss train: 3.979, val: 4.336 | iter time: 113.72 ms (step) remaining time: 0:02:47
Epoch 5 | iter 2800 step 2800 | loss train: 3.941, val: 4.336 | iter time: 113.71 ms (step) remaining time: 0:02:45
Validating ...
iter 2800: val loss 4.3300, val time: 4084.24 ms
Epoch 5 | iter 2810 step 2810 | loss train: 3.921, val: 4.330 | iter time: 112.84 ms (step) remaining time: 0:02:45
Epoch 5 | iter 2820 step 2820 | loss train: 3.974, val: 4.330 | iter time: 112.96 ms (step) remaining time: 0:02:43
Epoch 5 | iter 2830 step 2830 | loss train: 3.823, val: 4.330 | iter time: 112.88 ms (step) remaining time: 0:02:41
Epoch 5 | iter 2840 step 2840 | loss train: 3.910, val: 4.330 | iter time: 112.95 ms (step) remaining time: 0:02:40
Epoch 5 | iter 2850 step 2850 | loss train: 3.934, val: 4.330 | iter time: 113.26 ms (step) remaining time: 0:02:38
Epoch 5 | iter 2860 step 2860 | loss train: 4.046, val: 4.330 | iter time: 113.38 ms (step) remaining time: 0:02:36
Epoch 5 | iter 2870 step 2870 | loss train: 3.807, val: 4.330 | iter time: 113.71 ms (step) remaining time: 0:02:34
Epoch 5 | iter 2880 step 2880 | loss train: 4.008, val: 4.330 | iter time: 113.75 ms (step) remaining time: 0:02:33
Epoch 5 | iter 2890 step 2890 | loss train: 3.940, val: 4.330 | iter time: 113.66 ms (step) remaining time: 0:02:31
Epoch 5 | iter 2900 step 2900 | loss train: 3.918, val: 4.330 | iter time: 113.47 ms (step) remaining time: 0:02:29
Validating ...
iter 2900: val loss 4.3244, val time: 4119.05 ms
Epoch 5 | iter 2910 step 2910 | loss train: 3.966, val: 4.324 | iter time: 113.00 ms (step) remaining time: 0:02:29
Epoch 5 | iter 2920 step 2920 | loss train: 3.880, val: 4.324 | iter time: 112.89 ms (step) remaining time: 0:02:27
Epoch 5 | iter 2930 step 2930 | loss train: 3.887, val: 4.324 | iter time: 112.53 ms (step) remaining time: 0:02:25
Epoch 5 | iter 2940 step 2940 | loss train: 3.818, val: 4.324 | iter time: 112.89 ms (step) remaining time: 0:02:24
Epoch 5 | iter 2950 step 2950 | loss train: 3.898, val: 4.324 | iter time: 113.21 ms (step) remaining time: 0:02:22
Epoch 5 | iter 2960 step 2960 | loss train: 3.849, val: 4.324 | iter time: 113.43 ms (step) remaining time: 0:02:20
Epoch 5 | iter 2970 step 2970 | loss train: 3.930, val: 4.324 | iter time: 113.62 ms (step) remaining time: 0:02:18
Epoch 5 | iter 2980 step 2980 | loss train: 3.874, val: 4.324 | iter time: 113.84 ms (step) remaining time: 0:02:17
Epoch 5 | iter 2990 step 2990 | loss train: 3.911, val: 4.324 | iter time: 113.55 ms (step) remaining time: 0:02:15
Epoch 5 | iter 3000 step 3000 | loss train: 3.965, val: 4.324 | iter time: 113.80 ms (step) remaining time: 0:02:13
Validating ...
iter 3000: val loss 4.3175, val time: 4119.50 ms
Saving checkpoint to 'out/pretrain/custom_model_36_4M_pcfg_10_on_babylm/step-00003000/lit_model.pth'
Epoch 5 | iter 3010 step 3010 | loss train: 3.923, val: 4.318 | iter time: 113.29 ms (step) remaining time: 0:02:14
Epoch 5 | iter 3020 step 3020 | loss train: 3.950, val: 4.318 | iter time: 113.30 ms (step) remaining time: 0:02:12
Epoch 5 | iter 3030 step 3030 | loss train: 3.942, val: 4.318 | iter time: 113.33 ms (step) remaining time: 0:02:11
Epoch 5 | iter 3040 step 3040 | loss train: 3.925, val: 4.318 | iter time: 113.27 ms (step) remaining time: 0:02:09
Epoch 5 | iter 3050 step 3050 | loss train: 3.916, val: 4.318 | iter time: 113.34 ms (step) remaining time: 0:02:07
Epoch 5 | iter 3060 step 3060 | loss train: 3.910, val: 4.318 | iter time: 113.56 ms (step) remaining time: 0:02:06
Epoch 5 | iter 3070 step 3070 | loss train: 3.901, val: 4.318 | iter time: 113.50 ms (step) remaining time: 0:02:04
Epoch 5 | iter 3080 step 3080 | loss train: 3.954, val: 4.318 | iter time: 113.46 ms (step) remaining time: 0:02:02
Epoch 5 | iter 3090 step 3090 | loss train: 3.943, val: 4.318 | iter time: 113.53 ms (step) remaining time: 0:02:00
Epoch 5 | iter 3100 step 3100 | loss train: 3.875, val: 4.318 | iter time: 113.53 ms (step) remaining time: 0:01:59
Validating ...
iter 3100: val loss 4.3115, val time: 4102.88 ms
Epoch 5 | iter 3110 step 3110 | loss train: 3.901, val: 4.311 | iter time: 112.91 ms (step) remaining time: 0:01:58
Epoch 5 | iter 3120 step 3120 | loss train: 3.902, val: 4.311 | iter time: 112.71 ms (step) remaining time: 0:01:56
Epoch 5 | iter 3130 step 3130 | loss train: 3.858, val: 4.311 | iter time: 112.67 ms (step) remaining time: 0:01:54
Epoch 5 | iter 3140 step 3140 | loss train: 3.946, val: 4.311 | iter time: 112.84 ms (step) remaining time: 0:01:53
Epoch 5 | iter 3150 step 3150 | loss train: 3.967, val: 4.311 | iter time: 112.85 ms (step) remaining time: 0:01:51
Epoch 5 | iter 3160 step 3160 | loss train: 3.923, val: 4.311 | iter time: 112.78 ms (step) remaining time: 0:01:49
Epoch 5 | iter 3170 step 3170 | loss train: 3.952, val: 4.311 | iter time: 112.72 ms (step) remaining time: 0:01:48
Epoch 5 | iter 3180 step 3180 | loss train: 3.919, val: 4.311 | iter time: 112.86 ms (step) remaining time: 0:01:46
Epoch 5 | iter 3190 step 3190 | loss train: 3.929, val: 4.311 | iter time: 112.80 ms (step) remaining time: 0:01:44
Epoch 5 | iter 3200 step 3200 | loss train: 3.901, val: 4.311 | iter time: 112.86 ms (step) remaining time: 0:01:43
Validating ...
iter 3200: val loss 4.3065, val time: 4090.43 ms
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
Epoch 6 | iter 3210 step 3210 | loss train: 3.848, val: 4.306 | iter time: 113.00 ms (step) remaining time: 0:01:42
Epoch 6 | iter 3220 step 3220 | loss train: 3.741, val: 4.306 | iter time: 112.87 ms (step) remaining time: 0:01:40
Epoch 6 | iter 3230 step 3230 | loss train: 3.863, val: 4.306 | iter time: 112.81 ms (step) remaining time: 0:01:38
Epoch 6 | iter 3240 step 3240 | loss train: 3.819, val: 4.306 | iter time: 113.02 ms (step) remaining time: 0:01:37
Epoch 6 | iter 3250 step 3250 | loss train: 3.836, val: 4.306 | iter time: 112.76 ms (step) remaining time: 0:01:35
Epoch 6 | iter 3260 step 3260 | loss train: 3.820, val: 4.306 | iter time: 112.75 ms (step) remaining time: 0:01:33
Epoch 6 | iter 3270 step 3270 | loss train: 3.791, val: 4.306 | iter time: 112.71 ms (step) remaining time: 0:01:32
Epoch 6 | iter 3280 step 3280 | loss train: 3.895, val: 4.306 | iter time: 112.81 ms (step) remaining time: 0:01:30
Epoch 6 | iter 3290 step 3290 | loss train: 3.829, val: 4.306 | iter time: 112.76 ms (step) remaining time: 0:01:28
Epoch 6 | iter 3300 step 3300 | loss train: 3.842, val: 4.306 | iter time: 113.15 ms (step) remaining time: 0:01:27
Validating ...
iter 3300: val loss 4.3244, val time: 4090.56 ms
Epoch 6 | iter 3310 step 3310 | loss train: 3.875, val: 4.324 | iter time: 112.96 ms (step) remaining time: 0:01:26
Epoch 6 | iter 3320 step 3320 | loss train: 3.765, val: 4.324 | iter time: 112.80 ms (step) remaining time: 0:01:24
Epoch 6 | iter 3330 step 3330 | loss train: 3.766, val: 4.324 | iter time: 112.98 ms (step) remaining time: 0:01:22
Epoch 6 | iter 3340 step 3340 | loss train: 3.827, val: 4.324 | iter time: 112.96 ms (step) remaining time: 0:01:21
Epoch 6 | iter 3350 step 3350 | loss train: 3.840, val: 4.324 | iter time: 112.89 ms (step) remaining time: 0:01:19
Epoch 6 | iter 3360 step 3360 | loss train: 3.862, val: 4.324 | iter time: 112.75 ms (step) remaining time: 0:01:17
Epoch 6 | iter 3370 step 3370 | loss train: 3.805, val: 4.324 | iter time: 112.76 ms (step) remaining time: 0:01:16
Epoch 6 | iter 3380 step 3380 | loss train: 3.794, val: 4.324 | iter time: 112.79 ms (step) remaining time: 0:01:14
Epoch 6 | iter 3390 step 3390 | loss train: 3.821, val: 4.324 | iter time: 112.88 ms (step) remaining time: 0:01:12
Epoch 6 | iter 3400 step 3400 | loss train: 3.783, val: 4.324 | iter time: 112.77 ms (step) remaining time: 0:01:11
Validating ...
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
iter 3400: val loss 4.3259, val time: 4092.75 ms
Epoch 6 | iter 3410 step 3410 | loss train: 3.832, val: 4.326 | iter time: 112.74 ms (step) remaining time: 0:01:10
Epoch 6 | iter 3420 step 3420 | loss train: 3.807, val: 4.326 | iter time: 112.91 ms (step) remaining time: 0:01:08
Epoch 6 | iter 3430 step 3430 | loss train: 3.791, val: 4.326 | iter time: 112.71 ms (step) remaining time: 0:01:06
Epoch 6 | iter 3440 step 3440 | loss train: 3.860, val: 4.326 | iter time: 112.79 ms (step) remaining time: 0:01:05
Epoch 6 | iter 3450 step 3450 | loss train: 3.828, val: 4.326 | iter time: 112.88 ms (step) remaining time: 0:01:03
Epoch 6 | iter 3460 step 3460 | loss train: 3.771, val: 4.326 | iter time: 112.75 ms (step) remaining time: 0:01:01
Epoch 6 | iter 3470 step 3470 | loss train: 3.776, val: 4.326 | iter time: 112.83 ms (step) remaining time: 0:01:00
Epoch 6 | iter 3480 step 3480 | loss train: 3.793, val: 4.326 | iter time: 112.82 ms (step) remaining time: 0:00:58
Epoch 6 | iter 3490 step 3490 | loss train: 3.859, val: 4.326 | iter time: 112.85 ms (step) remaining time: 0:00:56
Epoch 6 | iter 3500 step 3500 | loss train: 3.806, val: 4.326 | iter time: 113.27 ms (step) remaining time: 0:00:55
Validating ...
iter 3500: val loss 4.3257, val time: 4091.86 ms
Epoch 6 | iter 3510 step 3510 | loss train: 3.825, val: 4.326 | iter time: 112.95 ms (step) remaining time: 0:00:53
Epoch 6 | iter 3520 step 3520 | loss train: 3.730, val: 4.326 | iter time: 112.94 ms (step) remaining time: 0:00:52
Epoch 6 | iter 3530 step 3530 | loss train: 3.760, val: 4.326 | iter time: 112.80 ms (step) remaining time: 0:00:50
Epoch 6 | iter 3540 step 3540 | loss train: 3.797, val: 4.326 | iter time: 112.84 ms (step) remaining time: 0:00:49
Epoch 6 | iter 3550 step 3550 | loss train: 3.762, val: 4.326 | iter time: 112.77 ms (step) remaining time: 0:00:47
Epoch 6 | iter 3560 step 3560 | loss train: 3.810, val: 4.326 | iter time: 112.93 ms (step) remaining time: 0:00:45
Epoch 6 | iter 3570 step 3570 | loss train: 3.781, val: 4.326 | iter time: 112.84 ms (step) remaining time: 0:00:44
Epoch 6 | iter 3580 step 3580 | loss train: 3.905, val: 4.326 | iter time: 112.83 ms (step) remaining time: 0:00:42
Epoch 6 | iter 3590 step 3590 | loss train: 3.744, val: 4.326 | iter time: 112.80 ms (step) remaining time: 0:00:40
Epoch 6 | iter 3600 step 3600 | loss train: 3.770, val: 4.326 | iter time: 113.38 ms (step) remaining time: 0:00:39
Validating ...
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
iter 3600: val loss 4.3232, val time: 4090.48 ms
Epoch 6 | iter 3610 step 3610 | loss train: 3.852, val: 4.323 | iter time: 112.85 ms (step) remaining time: 0:00:37
Epoch 6 | iter 3620 step 3620 | loss train: 3.795, val: 4.323 | iter time: 112.88 ms (step) remaining time: 0:00:36
Epoch 6 | iter 3630 step 3630 | loss train: 3.847, val: 4.323 | iter time: 112.89 ms (step) remaining time: 0:00:34
Epoch 6 | iter 3640 step 3640 | loss train: 3.808, val: 4.323 | iter time: 112.94 ms (step) remaining time: 0:00:33
Epoch 6 | iter 3650 step 3650 | loss train: 3.876, val: 4.323 | iter time: 112.82 ms (step) remaining time: 0:00:31
Epoch 6 | iter 3660 step 3660 | loss train: 3.957, val: 4.323 | iter time: 112.78 ms (step) remaining time: 0:00:29
Epoch 6 | iter 3670 step 3670 | loss train: 3.774, val: 4.323 | iter time: 113.46 ms (step) remaining time: 0:00:28
Epoch 6 | iter 3680 step 3680 | loss train: 3.803, val: 4.323 | iter time: 112.76 ms (step) remaining time: 0:00:26
Epoch 6 | iter 3690 step 3690 | loss train: 3.815, val: 4.323 | iter time: 112.68 ms (step) remaining time: 0:00:25
Epoch 6 | iter 3700 step 3700 | loss train: 3.878, val: 4.323 | iter time: 112.88 ms (step) remaining time: 0:00:23
Validating ...
iter 3700: val loss 4.3226, val time: 4080.17 ms
Epoch 6 | iter 3710 step 3710 | loss train: 3.741, val: 4.323 | iter time: 112.73 ms (step) remaining time: 0:00:22
Epoch 6 | iter 3720 step 3720 | loss train: 3.936, val: 4.323 | iter time: 112.81 ms (step) remaining time: 0:00:20
Epoch 6 | iter 3730 step 3730 | loss train: 3.991, val: 4.323 | iter time: 112.78 ms (step) remaining time: 0:00:18
Epoch 6 | iter 3740 step 3740 | loss train: 3.748, val: 4.323 | iter time: 112.79 ms (step) remaining time: 0:00:17
Epoch 6 | iter 3750 step 3750 | loss train: 3.856, val: 4.323 | iter time: 112.68 ms (step) remaining time: 0:00:15
Epoch 6 | iter 3760 step 3760 | loss train: 3.800, val: 4.323 | iter time: 113.00 ms (step) remaining time: 0:00:14
Epoch 6 | iter 3770 step 3770 | loss train: 3.773, val: 4.323 | iter time: 112.99 ms (step) remaining time: 0:00:12
Epoch 6 | iter 3780 step 3780 | loss train: 3.900, val: 4.323 | iter time: 112.80 ms (step) remaining time: 0:00:10
Epoch 6 | iter 3790 step 3790 | loss train: 3.804, val: 4.323 | iter time: 112.73 ms (step) remaining time: 0:00:09
Epoch 6 | iter 3800 step 3800 | loss train: 3.822, val: 4.323 | iter time: 112.79 ms (step) remaining time: 0:00:07
Validating ...
wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
iter 3800: val loss 4.3210, val time: 4090.09 ms
Epoch 6 | iter 3810 step 3810 | loss train: 3.854, val: 4.321 | iter time: 113.06 ms (step) remaining time: 0:00:06
Epoch 6 | iter 3820 step 3820 | loss train: 3.783, val: 4.321 | iter time: 112.91 ms (step) remaining time: 0:00:04
Epoch 6 | iter 3830 step 3830 | loss train: 3.818, val: 4.321 | iter time: 112.85 ms (step) remaining time: 0:00:03
Epoch 6 | iter 3840 step 3840 | loss train: 3.822, val: 4.321 | iter time: 112.94 ms (step) remaining time: 0:00:01
Validating ...
Final evaluation | val loss: 4.322 | val ppl: 75.325
Saving checkpoint to 'out/pretrain/custom_model_36_4M_pcfg_10_on_babylm/final/lit_model.pth'
Training time: 632.60s
Memory used: 12.03 GB
wandb: ERROR Error while calling W&B API: run pretrain-/qgionf1f not found during createRunFiles (<Response [404]>)
wandb: ERROR Error while calling W&B API: run pretrain-/qgionf1f not found during createRunFiles (<Response [404]>)
wandb: ERROR Error while calling W&B API: run pretrain-/qgionf1f not found during createRunFiles (<Response [404]>)
wandb: ERROR Error while calling W&B API: run pretrain-/qgionf1f not found during createRunFiles (<Response [404]>)
wandb: ERROR Error while calling W&B API: run pretrain-/qgionf1f not found during createRunFiles (<Response [404]>)
wandb: - 0.010 MB of 0.010 MB uploadedwandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)
wandb: \ 0.010 MB of 0.066 MB uploadedwandb: 
wandb: Run history:
wandb:                batches ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: device/batches_per_sec ‚ñà‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   device/flops_per_sec ‚ñà‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:   device/items_per_sec ‚ñà‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:             device/mfu ‚ñà‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: device/samples_per_sec ‚ñà‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñà‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñà‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                   iter ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:              iter_time ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ
wandb:          learning_rate ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                lengths ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:         remaining_time ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                samples ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                   time ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                 tokens ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:           total_tokens ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:    trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:               val_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                val_ppl ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                batches 3840
wandb: device/batches_per_sec 4.71095
wandb:   device/flops_per_sec 12337299407258.352
wandb:   device/items_per_sec 77184.19404
wandb:             device/mfu 0.31879
wandb: device/samples_per_sec 75.37519
wandb:                  epoch 5
wandb:                   iter 3840
wandb:              iter_time 0.11294
wandb:          learning_rate 4e-05
wandb:                lengths 62914560
wandb:                   loss 3.82171
wandb:         remaining_time 1.42359
wandb:                samples 61440
wandb:                   step 3840
wandb:                   time 607.39747
wandb:                 tokens 62914560
wandb:           total_tokens 62914560
wandb:    trainer/global_step 3849
wandb:               val_loss 4.32181
wandb:                val_ppl 75.32486
wandb: 
wandb: üöÄ View run copper-puddle-116 at: https://wandb.ai/pcfg_pretrain/pretrain-/runs/qgionf1f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/pcfg_pretrain/pretrain-
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_113436-qgionf1f/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
