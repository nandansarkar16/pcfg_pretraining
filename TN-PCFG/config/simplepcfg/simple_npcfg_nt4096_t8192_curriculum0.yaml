device: 0
save_dir : 'log'

data:
  train_file: 'data/wikitrain.pickle'
  val_file: 'data/wikival.pickle'
  test_file: 'data/wikitest.pickle'
  vocab_type: 'max_size'
  vocab_size: 50000 # Reduced to 10k which is the number of words that appears around 75 times in the BabyLM 10M train set, same as PTB
  min_freq: 2

model:
  model_name: 'SNPCFG'
  NT: 4096
  T: 8192
  s_dim: 512
  w_dim: 512
  h_dim: 512
  z_dim: 64
  r_dim: -1
  operation_space: normal  # log -> logsumexp, normal -> muliplication  placeholder
  entropy: False # placeholder


train:
  batch_size: 16
  max_epoch: 10 #originally 10
  max_len: 100
  #whether to use curriculum learning stragegy.
  curriculum: 0
  start_len: 30
  increment: 1
  patience: 5
  clip: 3

test:
  batch_size: 16
  max_tokens: 100 #originally 150
  bucket: 32
  decode: 'mbr'
  sampler: 'batch'

optimizer:
  name: 'adam'
  lr: 0.001 # originally 0.002
  mu: 0.9 # originally 0.75
  nu: 0.999 # originally 0.999

